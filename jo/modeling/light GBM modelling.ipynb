{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed7dc84c-c6b8-4209-a72a-62ad43f027dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 첫 번째 파일 불러오기 시작 ---\n",
      "'TS_csv.zip' ZIP 파일 내부 목록:\n",
      "- /tc_sgg_시군구코드.csv\n",
      "- /tc_codeb_코드B.csv\n",
      "- /tn_lodge_consume_his_숙박소비내역_B.csv\n",
      "- /tn_companion_info_동반자정보_B.csv\n",
      "- /tc_codea_코드A.csv\n",
      "- /tn_move_his_이동내역_B.csv\n",
      "- /tn_adv_consume_his_사전소비내역_B.csv\n",
      "- /tn_poi_master_POIMaster.csv\n",
      "- /tn_activity_consume_his_활동소비내역_B.csv\n",
      "- /tn_visit_area_info_방문지정보_B.csv\n",
      "- /tn_travel_여행_B.csv\n",
      "- /tn_mvmn_consume_his_이동수단소비내역_B.csv\n",
      "- /tn_tour_photo_관광사진_B.csv\n",
      "- /tn_traveller_master_여행객 Master_B.csv\n",
      "- /tn_activity_his_활동내역_B.csv\n",
      "\n",
      "'/tn_traveller_master_여행객 Master_B.csv' (Training) 파일이 성공적으로 불러와져 'df_traveller_ts_east'로 생성되었습니다.\n",
      "df_traveller_ts_east 상위 3행:\n",
      "  TRAVELER_ID  RESIDENCE_SGG_CD GENDER  AGE_GRP  EDU_NM  EDU_FNSH_SE  \\\n",
      "0     b009514                31      여       20       6            1   \n",
      "1     b015795                29      여       30       6            1   \n",
      "2     b004208                31      여       20       6            1   \n",
      "\n",
      "   MARR_STTS  FAMILY_MEMB  JOB_NM  JOB_ETC  ...  TRAVEL_STYL_7  TRAVEL_STYL_8  \\\n",
      "0          1            3     3.0      NaN  ...              2              4   \n",
      "1          1            1     3.0      NaN  ...              6              6   \n",
      "2          1            4     2.0      NaN  ...              6              5   \n",
      "\n",
      "   TRAVEL_STATUS_RESIDENCE  TRAVEL_STATUS_DESTINATION  \\\n",
      "0                    울산광역시                         경남   \n",
      "1                    광주광역시                         대구   \n",
      "2                    울산광역시                         경북   \n",
      "\n",
      "   TRAVEL_STATUS_ACCOMPANY      TRAVEL_STATUS_YMD  TRAVEL_MOTIVE_1  \\\n",
      "0              2인 여행(가족 외)  2022-10-30~2022-10-30                6   \n",
      "1                   나홀로 여행  2022-10-29~2022-10-29                8   \n",
      "2              2인 여행(가족 외)  2022-09-24~2022-09-25                3   \n",
      "\n",
      "   TRAVEL_MOTIVE_2  TRAVEL_MOTIVE_3  TRAVEL_COMPANIONS_NUM  \n",
      "0              2.0              NaN                      1  \n",
      "1              2.0              1.0                      0  \n",
      "2              7.0              5.0                      1  \n",
      "\n",
      "[3 rows x 36 columns]\n",
      "\n",
      "df_traveller_ts_east 행/열: (3200, 36)\n",
      "\n",
      "--- 첫첫 번째 파일 불러오기 완료 ---\n",
      "\n",
      "======================================================================\n",
      "\n",
      "--- 세 번째 파일 불러오기 시작 ---\n",
      "'TS_csv.zip' ZIP 파일 내부 목록:\n",
      "- /tn_move_his_이동내역_A.csv\n",
      "- /tc_codeb_코드B.csv\n",
      "- /tc_codea_코드A.csv\n",
      "- /tc_sgg_시군구코드.csv\n",
      "- /tn_adv_consume_his_사전소비내역_A.csv\n",
      "- /tn_visit_area_info_방문지정보_A.csv\n",
      "- /tn_companion_info_동반자정보_A.csv\n",
      "- /tn_activity_consume_his_활동소비내역_A.csv\n",
      "- /tn_activity_his_활동내역_A.csv\n",
      "- /tn_mvmn_consume_his_이동수단소비내역_A.csv\n",
      "- /tn_lodge_consume_his_숙박소비내역_A.csv\n",
      "- /tn_tour_photo_관광사진_A.csv\n",
      "- /tn_poi_master_POIMaster.csv\n",
      "- /tn_travel_여행_A.csv\n",
      "- /tn_traveller_master_여행객 Master_A.csv\n",
      "\n",
      "'/tn_traveller_master_여행객 Master_A.csv' (Training) 파일이 성공적으로 불러와져 'df_traveller_ts_sudo'로 생성되었습니다.\n",
      "df_traveller_ts_sudo 상위 3행:\n",
      "  TRAVELER_ID  RESIDENCE_SGG_CD GENDER  AGE_GRP  EDU_NM  EDU_FNSH_SE  \\\n",
      "0     b015583                41      여       20       6          1.0   \n",
      "1     a001105                30      남       30       6          1.0   \n",
      "2     a001673                41      여       30       4          1.0   \n",
      "\n",
      "   MARR_STTS  FAMILY_MEMB  JOB_NM  JOB_ETC  ...  TRAVEL_STYL_7  TRAVEL_STYL_8  \\\n",
      "0        1.0            5     3.0      NaN  ...              1              7   \n",
      "1        1.0            2     3.0      NaN  ...              6              5   \n",
      "2        2.0            3     NaN      1.0  ...              2              7   \n",
      "\n",
      "   TRAVEL_STATUS_RESIDENCE  TRAVEL_STATUS_DESTINATION  \\\n",
      "0                      경기도                         충남   \n",
      "1                    대전광역시                         경기   \n",
      "2                      경기도                         서울   \n",
      "\n",
      "   TRAVEL_STATUS_ACCOMPANY      TRAVEL_STATUS_YMD  TRAVEL_MOTIVE_1  \\\n",
      "0              2인 여행(가족 외)  2022-10-29~2022-10-30                1   \n",
      "1           3인 이상 여행(가족 외)  2022-09-03~2022-09-04                3   \n",
      "2                 자녀 동반 여행  2022-09-18~2022-09-19               10   \n",
      "\n",
      "   TRAVEL_MOTIVE_2  TRAVEL_MOTIVE_3  TRAVEL_COMPANIONS_NUM  \n",
      "0              NaN              NaN                      1  \n",
      "1              NaN              NaN                      7  \n",
      "2              7.0              1.0                      2  \n",
      "\n",
      "[3 rows x 36 columns]\n",
      "\n",
      "df_traveller_ts_sudo 행/열: (3200, 36)\n",
      "\n",
      "--- 세 번째 파일 불러오기 완료 ---\n",
      "\n",
      "======================================================================\n",
      "\n",
      "--- 두 번째 파일 불러오기 시작 ---\n",
      "'TS_csv.zip' ZIP 파일 내부 목록:\n",
      "- /tc_codeb_코드B.csv\n",
      "- /tn_companion_info_동반자정보_D.csv\n",
      "- /tc_codea_코드A.csv\n",
      "- /tn_poi_master_POIMaster.csv\n",
      "- /tn_adv_consume_his_사전소비내역_D.csv\n",
      "- /tn_mvmn_consume_his_이동수단소비내역_D.csv\n",
      "- /tn_tour_photo_관광사진_D.csv\n",
      "- /tn_activity_consume_his_활동소비내역_D.csv\n",
      "- /tn_activity_his_활동내역_D.csv\n",
      "- /tc_sgg_시군구코드.csv\n",
      "- /tn_travel_여행_D.csv\n",
      "- /tn_visit_area_info_방문지정보_D.csv\n",
      "- /tn_lodge_consume_his_숙박소비내역_D.csv\n",
      "- /tn_traveller_master_여행객 Master_D.csv\n",
      "- /tn_move_his_이동내역_D.csv\n",
      "\n",
      "'/tn_traveller_master_여행객 Master_D.csv' (Training) 파일이 성공적으로 불러와져 'df_traveller_ts_jeju'로 생성되었습니다.\n",
      "df_traveller_ts_jeju 상위 3행:\n",
      "  TRAVELER_ID  RESIDENCE_SGG_CD GENDER  AGE_GRP  EDU_NM  EDU_FNSH_SE  \\\n",
      "0     b011774                31      여       30       6            1   \n",
      "1     d009928                29      여       20       6            2   \n",
      "2     d010131                26      남       20       7            2   \n",
      "\n",
      "   MARR_STTS  FAMILY_MEMB  JOB_NM  JOB_ETC  ...  TRAVEL_STYL_7  TRAVEL_STYL_8  \\\n",
      "0          1            1     3.0      NaN  ...              2              6   \n",
      "1          1            4    12.0      NaN  ...              1              7   \n",
      "2          1            1    12.0      NaN  ...              2              7   \n",
      "\n",
      "   TRAVEL_STATUS_RESIDENCE  TRAVEL_STATUS_DESTINATION  \\\n",
      "0                    울산광역시                         경남   \n",
      "1                    광주광역시                         제주   \n",
      "2                    부산광역시                         제주   \n",
      "\n",
      "   TRAVEL_STATUS_ACCOMPANY      TRAVEL_STATUS_YMD  TRAVEL_MOTIVE_1  \\\n",
      "0                 2인 가족 여행  2022-10-25~2022-10-26                2   \n",
      "1              2인 여행(가족 외)  2022-10-30~2022-11-01                2   \n",
      "2                   나홀로 여행  2022-11-02~2022-11-04                5   \n",
      "\n",
      "   TRAVEL_MOTIVE_2  TRAVEL_MOTIVE_3  TRAVEL_COMPANIONS_NUM  \n",
      "0              1.0              5.0                      1  \n",
      "1              1.0              5.0                      1  \n",
      "2              2.0             10.0                      0  \n",
      "\n",
      "[3 rows x 36 columns]\n",
      "\n",
      "df_traveller_ts_jeju 행/열: (3200, 36)\n",
      "\n",
      "--- 네 번째 파일 불러오기 완료 ---\n",
      "\n",
      "======================================================================\n",
      "\n",
      "east <class 'pandas.core.frame.DataFrame'>\n",
      "west <class 'NoneType'>\n",
      "sudo <class 'pandas.core.frame.DataFrame'>\n",
      "jeju <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "\n",
    "\n",
    "# --- 2. 두 번째 파일 불러오기 및 데이터프레임 생성 ---\n",
    "print(\"--- 첫 번째 파일 불러오기 시작 ---\")\n",
    "\n",
    "# 두 번째 ZIP 파일의 전체 경로\n",
    "# 이 경로가 실제 파일이 있는 경로와 일치하는지 다시 확인해주세요.\n",
    "zip_file_path_ts = r'C:\\Users\\henaj\\Downloads\\278.국내 여행로그 데이터(동부권)\\01-1.정식개방데이터\\Training\\01.원천데이터\\TS_csv.zip'\n",
    "# ZIP 파일 안에 있는 CSV 파일 이름 (정확한지 다시 확인해주세요. 7-Zip에서 본 이름과 동일해야 합니다.)\n",
    "csv_file_name_in_zip_ts = '/tn_traveller_master_여행객 Master_B.csv'\n",
    "\n",
    "df_traveller_ts_east = None # 변수 초기화\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(zip_file_path_ts):\n",
    "        print(f\"오류: 두 번째 ZIP 파일이 지정된 경로에 존재하지 않습니다: {zip_file_path_ts}\")\n",
    "    else:\n",
    "        with zipfile.ZipFile(zip_file_path_ts, 'r') as zf_ts:\n",
    "            file_list_in_ts_zip = zf_ts.namelist()\n",
    "            print(f\"'{os.path.basename(zip_file_path_ts)}' ZIP 파일 내부 목록:\")\n",
    "            for name in file_list_in_ts_zip:\n",
    "                print(f\"- {name}\")\n",
    "\n",
    "            if csv_file_name_in_zip_ts in file_list_in_ts_zip:\n",
    "                with zf_ts.open(csv_file_name_in_zip_ts) as csv_file_ts:\n",
    "                    df_traveller_ts_east = pd.read_csv(io.BytesIO(csv_file_ts.read()), encoding='utf-8')\n",
    "                print(f\"\\n'{csv_file_name_in_zip_ts}' (Training) 파일이 성공적으로 불러와져 'df_traveller_ts_east'로 생성되었습니다.\")\n",
    "                print(f\"df_traveller_ts_east 상위 3행:\\n{df_traveller_ts_east.head(3)}\\n\")\n",
    "                print(f\"df_traveller_ts_east 행/열: {df_traveller_ts_east.shape}\\n\")\n",
    "            else:\n",
    "                print(f\"오류: '{os.path.basename(zip_file_path_ts)}' ZIP 파일 안에 지정된 CSV 파일 ('{csv_file_name_in_zip_ts}')을 찾을 수 없습니다.\")\n",
    "\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"오류: '{zip_file_path_ts}' 파일이 유효한 ZIP 파일이 아니거나 손상되었습니다.\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"오류: '{csv_file_name_in_zip_ts}' 파일을 읽는 중 인코딩 문제가 발생했습니다. (Training) - {e}\")\n",
    "    print(\"encoding='utf-8' 대신 'cp949' 또는 'euc-kr'을 시도해보세요.\")\n",
    "except Exception as e:\n",
    "    print(f\"두 번째 파일 처리 중 알 수 없는 오류가 발생했습니다: {e}\")\n",
    "\n",
    "print(\"--- 첫첫 번째 파일 불러오기 완료 ---\\n\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --- 2. 두 번째 파일 불러오기 및 데이터프레임 생성 ---\n",
    "print(\"--- 세 번째 파일 불러오기 시작 ---\")\n",
    "\n",
    "# 두 번째 ZIP 파일의 전체 경로\n",
    "# 이 경로가 실제 파일이 있는 경로와 일치하는지 다시 확인해주세요.\n",
    "zip_file_path_ts = r'C:\\Users\\henaj\\Downloads\\277.국내 여행로그 데이터(수도권)\\01-1.정식개방데이터\\Training\\01.원천데이터\\TS_csv.zip'\n",
    "# ZIP 파일 안에 있는 CSV 파일 이름 (정확한지 다시 확인해주세요. 7-Zip에서 본 이름과 동일해야 합니다.)\n",
    "csv_file_name_in_zip_ts = '/tn_traveller_master_여행객 Master_A.csv'\n",
    "\n",
    "df_traveller_ts_sudo = None # 변수 초기화\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(zip_file_path_ts):\n",
    "        print(f\"오류: 두 번째 ZIP 파일이 지정된 경로에 존재하지 않습니다: {zip_file_path_ts}\")\n",
    "    else:\n",
    "        with zipfile.ZipFile(zip_file_path_ts, 'r') as zf_ts:\n",
    "            file_list_in_ts_zip = zf_ts.namelist()\n",
    "            print(f\"'{os.path.basename(zip_file_path_ts)}' ZIP 파일 내부 목록:\")\n",
    "            for name in file_list_in_ts_zip:\n",
    "                print(f\"- {name}\")\n",
    "\n",
    "            if csv_file_name_in_zip_ts in file_list_in_ts_zip:\n",
    "                with zf_ts.open(csv_file_name_in_zip_ts) as csv_file_ts:\n",
    "                    df_traveller_ts_sudo = pd.read_csv(io.BytesIO(csv_file_ts.read()), encoding='utf-8')\n",
    "                print(f\"\\n'{csv_file_name_in_zip_ts}' (Training) 파일이 성공적으로 불러와져 'df_traveller_ts_sudo'로 생성되었습니다.\")\n",
    "                print(f\"df_traveller_ts_sudo 상위 3행:\\n{df_traveller_ts_sudo.head(3)}\\n\")\n",
    "                print(f\"df_traveller_ts_sudo 행/열: {df_traveller_ts_sudo.shape}\\n\")\n",
    "            else:\n",
    "                print(f\"오류: '{os.path.basename(zip_file_path_ts)}' ZIP 파일 안에 지정된 CSV 파일 ('{csv_file_name_in_zip_ts}')을 찾을 수 없습니다.\")\n",
    "\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"오류: '{zip_file_path_ts}' 파일이 유효한 ZIP 파일이 아니거나 손상되었습니다.\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"오류: '{csv_file_name_in_zip_ts}' 파일을 읽는 중 인코딩 문제가 발생했습니다. (Training) - {e}\")\n",
    "    print(\"encoding='utf-8' 대신 'cp949' 또는 'euc-kr'을 시도해보세요.\")\n",
    "except Exception as e:\n",
    "    print(f\"두 번째 파일 처리 중 알 수 없는 오류가 발생했습니다: {e}\")\n",
    "\n",
    "print(\"--- 세 번째 파일 불러오기 완료 ---\\n\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    " # ------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --- 2. 네 번째 파일 불러오기 및 데이터프레임 생성 ---\n",
    "print(\"--- 두 번째 파일 불러오기 시작 ---\")\n",
    "\n",
    "# 네 번째 ZIP 파일의 전체 경로\n",
    "# 이 경로가 실제 파일이 있는 경로와 일치하는지 다시 확인해주세요.\n",
    "zip_file_path_ts = r'C:\\Users\\henaj\\Downloads\\280.국내 여행로그 데이터(제주도 및 도서지역)\\01-1.정식개방데이터\\Training\\01.원천데이터\\TS_csv.zip'\n",
    "# ZIP 파일 안에 있는 CSV 파일 이름 (정확한지 다시 확인해주세요. 7-Zip에서 본 이름과 동일해야 합니다.)\n",
    "csv_file_name_in_zip_ts = '/tn_traveller_master_여행객 Master_D.csv'\n",
    "\n",
    "df_traveller_ts_west = None # 변수 초기화\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(zip_file_path_ts):\n",
    "        print(f\"오류: 두 번째 ZIP 파일이 지정된 경로에 존재하지 않습니다: {zip_file_path_ts}\")\n",
    "    else:\n",
    "        with zipfile.ZipFile(zip_file_path_ts, 'r') as zf_ts:\n",
    "            file_list_in_ts_zip = zf_ts.namelist()\n",
    "            print(f\"'{os.path.basename(zip_file_path_ts)}' ZIP 파일 내부 목록:\")\n",
    "            for name in file_list_in_ts_zip:\n",
    "                print(f\"- {name}\")\n",
    "\n",
    "            if csv_file_name_in_zip_ts in file_list_in_ts_zip:\n",
    "                with zf_ts.open(csv_file_name_in_zip_ts) as csv_file_ts:\n",
    "                    df_traveller_ts_jeju = pd.read_csv(io.BytesIO(csv_file_ts.read()), encoding='utf-8')\n",
    "                print(f\"\\n'{csv_file_name_in_zip_ts}' (Training) 파일이 성공적으로 불러와져 'df_traveller_ts_jeju'로 생성되었습니다.\")\n",
    "                print(f\"df_traveller_ts_jeju 상위 3행:\\n{df_traveller_ts_jeju.head(3)}\\n\")\n",
    "                print(f\"df_traveller_ts_jeju 행/열: {df_traveller_ts_jeju.shape}\\n\")\n",
    "            else:\n",
    "                print(f\"오류: '{os.path.basename(zip_file_path_ts)}' ZIP 파일 안에 지정된 CSV 파일 ('{csv_file_name_in_zip_ts}')을 찾을 수 없습니다.\")\n",
    "\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"오류: '{zip_file_path_ts}' 파일이 유효한 ZIP 파일이 아니거나 손상되었습니다.\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"오류: '{csv_file_name_in_zip_ts}' 파일을 읽는 중 인코딩 문제가 발생했습니다. (Training) - {e}\")\n",
    "    print(\"encoding='utf-8' 대신 'cp949' 또는 'euc-kr'을 시도해보세요.\")\n",
    "except Exception as e:\n",
    "    print(f\"네 번째 파일 처리 중 알 수 없는 오류가 발생했습니다: {e}\")\n",
    "\n",
    "print(\"--- 네 번째 파일 불러오기 완료 ---\\n\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "for name, df in {\n",
    "    'east': df_traveller_ts_east,\n",
    "    'west': df_traveller_ts_west,\n",
    "    'sudo': df_traveller_ts_sudo,\n",
    "    'jeju': df_traveller_ts_jeju\n",
    "}.items():\n",
    "    print(name, type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48cc31fa-59f0-4048-bb34-c05b008fb707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 두 번째 파일 불러오기 시작 ---\n",
      "'TS_csv.zip' ZIP 파일 내부 목록:\n",
      "- /tc_sgg_시군구코드.csv\n",
      "- /tc_codea_코드A.csv\n",
      "- /tn_activity_his_활동내역_C.csv\n",
      "- /tn_lodge_consume_his_숙박소비내역_C.csv\n",
      "- /tc_codeb_코드B.csv\n",
      "- /tn_activity_consume_his_활동소비내역_C.csv\n",
      "- /tn_mvmn_consume_his_이동수단소비내역_C.csv\n",
      "- /tn_adv_consume_his_사전소비내역_C.csv\n",
      "- /tn_companion_info_동반자정보_C.csv\n",
      "- /tn_visit_area_info_방문지정보_C.csv\n",
      "- /tn_traveller_master_여행객 Master_C.csv\n",
      "- /tn_tour_photo_관광사진_C.csv\n",
      "- /tn_travel_여행_C.csv\n",
      "- /tn_poi_master_POIMaster.csv\n",
      "- /tn_move_his_이동내역_C.csv\n",
      "\n",
      "'/tn_traveller_master_여행객 Master_C.csv' (Training) 파일이 성공적으로 불러와져 'df_traveller_ts_west'로 생성되었습니다.\n",
      "df_traveller_ts_west 상위 3행:\n",
      "  TRAVELER_ID  RESIDENCE_SGG_CD GENDER  AGE_GRP  EDU_NM  EDU_FNSH_SE  \\\n",
      "0     c003867                48      여       20       5            1   \n",
      "1     c000679                11      여       20       6            1   \n",
      "2     c013674                11      여       20       6            1   \n",
      "\n",
      "   MARR_STTS  FAMILY_MEMB  JOB_NM  JOB_ETC  ...  TRAVEL_STYL_7  TRAVEL_STYL_8  \\\n",
      "0          1            1     2.0      NaN  ...              2              5   \n",
      "1          1            1     2.0      NaN  ...              7              7   \n",
      "2          1            2     3.0      NaN  ...              1              7   \n",
      "\n",
      "   TRAVEL_STATUS_RESIDENCE  TRAVEL_STATUS_DESTINATION  \\\n",
      "0                     경상남도                         전북   \n",
      "1                    서울특별시                         전북   \n",
      "2                    서울특별시                         충남   \n",
      "\n",
      "   TRAVEL_STATUS_ACCOMPANY      TRAVEL_STATUS_YMD  TRAVEL_MOTIVE_1  \\\n",
      "0                   나홀로 여행  2022-09-29~2022-09-30              1.0   \n",
      "1              2인 여행(가족 외)  2022-08-21~2022-08-22              5.0   \n",
      "2                 부모 동반 여행  2022-11-05~2022-11-05              1.0   \n",
      "\n",
      "   TRAVEL_MOTIVE_2  TRAVEL_MOTIVE_3  TRAVEL_COMPANIONS_NUM  \n",
      "0              7.0              2.0                      0  \n",
      "1              3.0              6.0                      1  \n",
      "2              8.0              3.0                      3  \n",
      "\n",
      "[3 rows x 36 columns]\n",
      "\n",
      "df_traveller_ts_west 행/열: (3200, 36)\n",
      "\n",
      "--- 두 번째 파일 불러오기 완료 ---\n",
      "\n",
      "======================================================================\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# --- 2. 두 번째 파일 불러오기 및 데이터프레임 생성 ---\n",
    "print(\"--- 두 번째 파일 불러오기 시작 ---\")\n",
    "\n",
    "# 두 번째 ZIP 파일의 전체 경로\n",
    "# 이 경로가 실제 파일이 있는 경로와 일치하는지 다시 확인해주세요.\n",
    "zip_file_path_ts = r'C:\\Users\\henaj\\Downloads\\279.국내 여행로그 데이터(서부권)\\01-1.정식개방데이터\\Training\\01.원천데이터\\TS_csv.zip'\n",
    "# ZIP 파일 안에 있는 CSV 파일 이름 (정확한지 다시 확인해주세요. 7-Zip에서 본 이름과 동일해야 합니다.)\n",
    "csv_file_name_in_zip_ts = '/tn_traveller_master_여행객 Master_C.csv'\n",
    "\n",
    "df_traveller_ts_west = None # 변수 초기화\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(zip_file_path_ts):\n",
    "        print(f\"오류: 두 번째 ZIP 파일이 지정된 경로에 존재하지 않습니다: {zip_file_path_ts}\")\n",
    "    else:\n",
    "        with zipfile.ZipFile(zip_file_path_ts, 'r') as zf_ts:\n",
    "            file_list_in_ts_zip = zf_ts.namelist()\n",
    "            print(f\"'{os.path.basename(zip_file_path_ts)}' ZIP 파일 내부 목록:\")\n",
    "            for name in file_list_in_ts_zip:\n",
    "                print(f\"- {name}\")\n",
    "\n",
    "            if csv_file_name_in_zip_ts in file_list_in_ts_zip:\n",
    "                with zf_ts.open(csv_file_name_in_zip_ts) as csv_file_ts:\n",
    "                    df_traveller_ts_west = pd.read_csv(io.BytesIO(csv_file_ts.read()), encoding='utf-8')\n",
    "                print(f\"\\n'{csv_file_name_in_zip_ts}' (Training) 파일이 성공적으로 불러와져 'df_traveller_ts_west'로 생성되었습니다.\")\n",
    "                print(f\"df_traveller_ts_west 상위 3행:\\n{df_traveller_ts_west.head(3)}\\n\")\n",
    "                print(f\"df_traveller_ts_west 행/열: {df_traveller_ts_west.shape}\\n\")\n",
    "            else:\n",
    "                print(f\"오류: '{os.path.basename(zip_file_path_ts)}' ZIP 파일 안에 지정된 CSV 파일 ('{csv_file_name_in_zip_ts}')을 찾을 수 없습니다.\")\n",
    "\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"오류: '{zip_file_path_ts}' 파일이 유효한 ZIP 파일이 아니거나 손상되었습니다.\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"오류: '{csv_file_name_in_zip_ts}' 파일을 읽는 중 인코딩 문제가 발생했습니다. (Training) - {e}\")\n",
    "    print(\"encoding='utf-8' 대신 'cp949' 또는 'euc-kr'을 시도해보세요.\")\n",
    "except Exception as e:\n",
    "    print(f\"두 번째 파일 처리 중 알 수 없는 오류가 발생했습니다: {e}\")\n",
    "\n",
    "print(\"--- 두 번째 파일 불러오기 완료 ---\\n\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(type(df_traveller_ts_west))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f543177-267b-403c-bd97-30f9c55f3795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 데이터프레임 정보:\n",
      "east: shape=(3200, 36), columns=['TRAVELER_ID', 'RESIDENCE_SGG_CD', 'GENDER', 'AGE_GRP', 'EDU_NM']...\n",
      "west: shape=(3200, 36), columns=['TRAVELER_ID', 'RESIDENCE_SGG_CD', 'GENDER', 'AGE_GRP', 'EDU_NM']...\n",
      "sudo: shape=(3200, 36), columns=['TRAVELER_ID', 'RESIDENCE_SGG_CD', 'GENDER', 'AGE_GRP', 'EDU_NM']...\n",
      "jeju: shape=(3200, 36), columns=['TRAVELER_ID', 'RESIDENCE_SGG_CD', 'GENDER', 'AGE_GRP', 'EDU_NM']...\n"
     ]
    }
   ],
   "source": [
    "print(\"각 데이터프레임 정보:\")\n",
    "for name, df in [('east', df_traveller_ts_east), ('west', df_traveller_ts_west), \n",
    "                 ('sudo', df_traveller_ts_sudo), ('jeju', df_traveller_ts_jeju)]:\n",
    "    print(f\"{name}: shape={df.shape}, columns={list(df.columns)[:5]}...\")  # 처음 5개 열만 출력재시도Claude는 아직 생성한 코드를 실행할 수 없습니다.Claude는 실수를 할 수 있습니다. 응답을 반드시 다시 확인해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5eb0492-8fe5-47ea-8ce7-7cc36a9af1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 최종 데이터프레임 (df_traveller_ts)의 상위 5개 행:\n",
      "  TRAVELER_ID  RESIDENCE_SGG_CD GENDER  AGE_GRP  EDU_NM  EDU_FNSH_SE  \\\n",
      "0     b009514                31      여       20       6          1.0   \n",
      "1     b015795                29      여       30       6          1.0   \n",
      "2     b004208                31      여       20       6          1.0   \n",
      "3     b006840                11      여       30       6          1.0   \n",
      "4     b002386                26      남       40       4          1.0   \n",
      "\n",
      "   MARR_STTS  FAMILY_MEMB  JOB_NM  JOB_ETC  ...  TRAVEL_STYL_7  TRAVEL_STYL_8  \\\n",
      "0        1.0            3     3.0      NaN  ...              2              4   \n",
      "1        1.0            1     3.0      NaN  ...              6              6   \n",
      "2        1.0            4     2.0      NaN  ...              6              5   \n",
      "3        1.0            4     3.0      NaN  ...              1              7   \n",
      "4        2.0            4     5.0      NaN  ...              7              7   \n",
      "\n",
      "   TRAVEL_STATUS_RESIDENCE  TRAVEL_STATUS_DESTINATION  \\\n",
      "0                    울산광역시                         경남   \n",
      "1                    광주광역시                         대구   \n",
      "2                    울산광역시                         경북   \n",
      "3                    서울특별시                         강원   \n",
      "4                    부산광역시                         울산   \n",
      "\n",
      "   TRAVEL_STATUS_ACCOMPANY      TRAVEL_STATUS_YMD  TRAVEL_MOTIVE_1  \\\n",
      "0              2인 여행(가족 외)  2022-10-30~2022-10-30              6.0   \n",
      "1                   나홀로 여행  2022-10-29~2022-10-29              8.0   \n",
      "2              2인 여행(가족 외)  2022-09-24~2022-09-25              3.0   \n",
      "3                   나홀로 여행  2022-10-05~2022-10-05              5.0   \n",
      "4                 자녀 동반 여행  2022-08-22~2022-08-23              3.0   \n",
      "\n",
      "   TRAVEL_MOTIVE_2  TRAVEL_MOTIVE_3  TRAVEL_COMPANIONS_NUM  \n",
      "0              2.0              NaN                      1  \n",
      "1              2.0              1.0                      0  \n",
      "2              7.0              5.0                      1  \n",
      "3              7.0              1.0                      1  \n",
      "4              7.0              2.0                      3  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "(12799, 36)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 가정: 네 개의 데이터프레임이 이미 로드되어 있다고 가정합니다.\n",
    "# df_traveller_ts_east, df_traveller_ts_west, df_traveller_ts_sudo, df_traveller_ts_jeju\n",
    "\n",
    "# 1. 데이터프레임 합치기 및 전처리\n",
    "# pd.concat()을 사용하여 데이터프레임을 합칩니다.\n",
    "# 첫 번째 데이터프레임(df_traveller_ts_east)의 열 이름을 기준으로 나머지 데이터프레임을 합치면\n",
    "# 열 이름 중복 문제를 피할 수 있습니다.\n",
    "# ignore_index=True를 설정하여 합쳐진 데이터프레임의 인덱스가 0부터 새로 시작하도록 합니다.\n",
    "df_traveller_ts = pd.concat([df_traveller_ts_east, df_traveller_ts_west, df_traveller_ts_sudo, df_traveller_ts_jeju], ignore_index=True)\n",
    "\n",
    "# 2. 결측치 처리 (특정 열에 결측치가 하나라도 있는 행 삭제)\n",
    "# dropna() 함수를 사용하여 특정 열(subset)에 NaN 값이 있는 행을 삭제합니다.\n",
    "# inplace=True를 설정하여 원래 데이터프레임에 변경사항을 바로 적용합니다.\n",
    "columns_to_check = ['TRAVEL_STATUS_ACCOMPANY', 'TRAVEL_COMPANIONS_NUM', 'INCOME', 'TRAVEL_STYL_1', 'TRAVEL_MOTIVE_1', 'TRAVEL_STATUS_DESTINATION']\n",
    "df_traveller_ts.dropna(subset=columns_to_check, inplace=True)\n",
    "\n",
    "# 3. 특정 열의 값 범위에 맞지 않는 행 삭제\n",
    "# 각 열에 대해 주어진 범위에 해당하는지 확인하고, 해당하지 않는 행을 삭제합니다.\n",
    "# ~를 사용하여 조건을 만족하지 않는 행을 선택하고, df.index를 통해 해당 행의 인덱스를 삭제합니다.\n",
    "# isin() 메서드는 주어진 리스트에 값이 포함되어 있는지 확인하는 데 유용합니다.\n",
    "\n",
    "# INCOME 열 처리 (1~12 정수)\n",
    "income_range = list(range(1, 13))\n",
    "df_traveller_ts = df_traveller_ts[df_traveller_ts['INCOME'].isin(income_range)]\n",
    "\n",
    "# TRAVEL_STYL_1 열 처리 (1~7 정수)\n",
    "travel_style_range = list(range(1, 8))\n",
    "df_traveller_ts = df_traveller_ts[df_traveller_ts['TRAVEL_STYL_1'].isin(travel_style_range)]\n",
    "\n",
    "# TRAVEL_MOTIVE_1 열 처리 (1~10 정수)\n",
    "travel_motive_range = list(range(1, 11))\n",
    "df_traveller_ts = df_traveller_ts[df_traveller_ts['TRAVEL_MOTIVE_1'].isin(travel_motive_range)]\n",
    "\n",
    "# 모든 작업이 완료된 후 데이터프레임의 첫 5행을 출력하여 확인\n",
    "print(\"전처리 후 최종 데이터프레임 (df_traveller_ts)의 상위 5개 행:\")\n",
    "print(df_traveller_ts.head())\n",
    "print(df_traveller_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4832fc8f-763a-485c-821e-05f7eba3ca79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 첫 번째 파일 불러오기 시작 ---\n",
      "'VS_csv.zip' ZIP 파일 내부 목록:\n",
      "- /tn_travel_여행_B.csv\n",
      "- /tn_lodge_consume_his_숙박소비내역_B.csv\n",
      "- /tn_tour_photo_관광사진_B.csv\n",
      "- /tn_activity_his_활동내역_B.csv\n",
      "- /tn_companion_info_동반자정보_B.csv\n",
      "- /tc_codea_코드A.csv\n",
      "- /tn_activity_consume_his_활동소비내역_B.csv\n",
      "- /tn_mvmn_consume_his_이동수단소비내역_B.csv\n",
      "- /tn_adv_consume_his_사전소비내역_B.csv\n",
      "- /tn_move_his_이동내역_B.csv\n",
      "- /tn_traveller_master_여행객 Master_B.csv\n",
      "- /tn_poi_master_POIMaster.csv\n",
      "- /tc_sgg_시군구코드.csv\n",
      "- /tc_codeb_코드B.csv\n",
      "- /tn_visit_area_info_방문지정보_B.csv\n",
      "\n",
      "'/tn_traveller_master_여행객 Master_B.csv' (Validation) 파일이 성공적으로 불러와져 'df_traveller_vs_east'로 생성되었습니다.\n",
      "df_traveller_vs_east 상위 3행:\n",
      "  TRAVELER_ID  RESIDENCE_SGG_CD GENDER  AGE_GRP  EDU_NM  EDU_FNSH_SE  \\\n",
      "0     b000513                41      여       20       6            1   \n",
      "1     b001325                11      여       30       6            1   \n",
      "2     b005349                48      여       20       8            2   \n",
      "\n",
      "   MARR_STTS  FAMILY_MEMB  JOB_NM  JOB_ETC  ...  TRAVEL_STYL_7  TRAVEL_STYL_8  \\\n",
      "0          1            1     NaN      3.0  ...              6              7   \n",
      "1          2            2     3.0      NaN  ...              5              4   \n",
      "2          1            3    12.0      NaN  ...              1              3   \n",
      "\n",
      "   TRAVEL_STATUS_RESIDENCE  TRAVEL_STATUS_DESTINATION  \\\n",
      "0                      경기도                         경북   \n",
      "1                    서울특별시                         강원   \n",
      "2                    서울특별시                         경남   \n",
      "\n",
      "   TRAVEL_STATUS_ACCOMPANY      TRAVEL_STATUS_YMD  TRAVEL_MOTIVE_1  \\\n",
      "0                   나홀로 여행  2022-08-26~2022-08-28                2   \n",
      "1                 2인 가족 여행  2022-08-21~2022-08-22                2   \n",
      "2              2인 여행(가족 외)  2022-10-22~2022-10-23                3   \n",
      "\n",
      "   TRAVEL_MOTIVE_2  TRAVEL_MOTIVE_3  TRAVEL_COMPANIONS_NUM  \n",
      "0              NaN              NaN                      2  \n",
      "1              NaN              NaN                      1  \n",
      "2              NaN              NaN                      1  \n",
      "\n",
      "[3 rows x 36 columns]\n",
      "\n",
      "df_traveller_vs_east 행/열: (400, 36)\n",
      "\n",
      "--- 첫 번째 파일 불러오기 완료 ---\n",
      "\n",
      "======================================================================\n",
      "\n",
      "--- 두 번째 파일 불러오기 시작 ---\n",
      "'VS_csv.zip' ZIP 파일 내부 목록:\n",
      "- /tn_poi_master_POIMaster.csv\n",
      "- /tn_activity_his_활동내역_C.csv\n",
      "- /tc_codea_코드A.csv\n",
      "- /tn_adv_consume_his_사전소비내역_C.csv\n",
      "- /tn_lodge_consume_his_숙박소비내역_C.csv\n",
      "- /tn_traveller_master_여행객 Master_C.csv\n",
      "- /tn_travel_여행_C.csv\n",
      "- /tn_companion_info_동반자정보_C.csv\n",
      "- /tn_tour_photo_관광사진_C.csv\n",
      "- /tc_codeb_코드B.csv\n",
      "- /tn_activity_consume_his_활동소비내역_C.csv\n",
      "- /tc_sgg_시군구코드.csv\n",
      "- /tn_mvmn_consume_his_이동수단소비내역_C.csv\n",
      "- /tn_move_his_이동내역_C.csv\n",
      "- /tn_visit_area_info_방문지정보_C.csv\n",
      "\n",
      "'/tn_traveller_master_여행객 Master_C.csv' (Validation) 파일이 성공적으로 불러와져 'df_traveller_vs_west'로 생성되었습니다.\n",
      "df_traveller_vs_west 상위 3행:\n",
      "  TRAVELER_ID  RESIDENCE_SGG_CD GENDER  AGE_GRP  EDU_NM  EDU_FNSH_SE  \\\n",
      "0     a006157                41      남       20       5            1   \n",
      "1     c008192                47      여       20       6            2   \n",
      "2     c007450                41      남       30       7            1   \n",
      "\n",
      "   MARR_STTS  FAMILY_MEMB  JOB_NM  JOB_ETC  ...  TRAVEL_STYL_7  TRAVEL_STYL_8  \\\n",
      "0          1            3     1.0      NaN  ...              5              7   \n",
      "1          1            4    12.0      NaN  ...              1              6   \n",
      "2          1            1    10.0      NaN  ...              4              4   \n",
      "\n",
      "   TRAVEL_STATUS_RESIDENCE  TRAVEL_STATUS_DESTINATION  \\\n",
      "0                      경기도                         경기   \n",
      "1                     경상북도                         충북   \n",
      "2                      경기도                         전북   \n",
      "\n",
      "   TRAVEL_STATUS_ACCOMPANY      TRAVEL_STATUS_YMD  TRAVEL_MOTIVE_1  \\\n",
      "0                   나홀로 여행  2022-10-09~2022-10-10                1   \n",
      "1                   나홀로 여행  2022-10-28~2022-10-29                5   \n",
      "2              2인 여행(가족 외)  2022-10-21~2022-10-22                1   \n",
      "\n",
      "   TRAVEL_MOTIVE_2  TRAVEL_MOTIVE_3  TRAVEL_COMPANIONS_NUM  \n",
      "0              2.0              4.0                      0  \n",
      "1              1.0              7.0                      0  \n",
      "2              7.0              NaN                      1  \n",
      "\n",
      "[3 rows x 36 columns]\n",
      "\n",
      "df_traveller_vs_west 행/열: (400, 36)\n",
      "\n",
      "--- 첫 번째 파일 불러오기 완료 ---\n",
      "\n",
      "======================================================================\n",
      "\n",
      "--- 첫 번째 파일 불러오기 시작 ---\n",
      "'VS_csv.zip' ZIP 파일 내부 목록:\n",
      "- /tc_codea_코드A.csv\n",
      "- /tc_codeb_코드B.csv\n",
      "- /tn_tour_photo_관광사진_A.csv\n",
      "- /tn_activity_his_활동내역_A.csv\n",
      "- /tn_lodge_consume_his_숙박소비내역_A.csv\n",
      "- /tn_mvmn_consume_his_이동수단소비내역_A.csv\n",
      "- /tn_companion_info_동반자정보_A.csv\n",
      "- /tn_travel_여행_A.csv\n",
      "- /tn_move_his_이동내역_A.csv\n",
      "- /tn_poi_master_POIMaster.csv\n",
      "- /tn_adv_consume_his_사전소비내역_A.csv\n",
      "- /tn_traveller_master_여행객 Master_A.csv\n",
      "- /tn_activity_consume_his_활동소비내역_A.csv\n",
      "- /tc_sgg_시군구코드.csv\n",
      "- /tn_visit_area_info_방문지정보_A.csv\n",
      "\n",
      "'/tn_traveller_master_여행객 Master_A.csv' (Validation) 파일이 성공적으로 불러와져 'df_traveller_vs_sudo'로 생성되었습니다.\n",
      "df_traveller_vs_sudo 상위 3행:\n",
      "  TRAVELER_ID  RESIDENCE_SGG_CD GENDER  AGE_GRP  EDU_NM  EDU_FNSH_SE  \\\n",
      "0     a006157                41      남       20       5            1   \n",
      "1     c008192                47      여       20       6            2   \n",
      "2     c007450                41      남       30       7            1   \n",
      "\n",
      "   MARR_STTS  FAMILY_MEMB  JOB_NM  JOB_ETC  ...  TRAVEL_STYL_7  TRAVEL_STYL_8  \\\n",
      "0          1            3     1.0      NaN  ...              5              7   \n",
      "1          1            4    12.0      NaN  ...              1              6   \n",
      "2          1            1    10.0      NaN  ...              4              4   \n",
      "\n",
      "   TRAVEL_STATUS_RESIDENCE  TRAVEL_STATUS_DESTINATION  \\\n",
      "0                      경기도                         경기   \n",
      "1                     경상북도                         충북   \n",
      "2                      경기도                         전북   \n",
      "\n",
      "   TRAVEL_STATUS_ACCOMPANY      TRAVEL_STATUS_YMD  TRAVEL_MOTIVE_1  \\\n",
      "0                   나홀로 여행  2022-10-09~2022-10-10                1   \n",
      "1                   나홀로 여행  2022-10-28~2022-10-29                5   \n",
      "2              2인 여행(가족 외)  2022-10-21~2022-10-22                1   \n",
      "\n",
      "   TRAVEL_MOTIVE_2  TRAVEL_MOTIVE_3  TRAVEL_COMPANIONS_NUM  \n",
      "0              2.0              4.0                      0  \n",
      "1              1.0              7.0                      0  \n",
      "2              7.0              NaN                      1  \n",
      "\n",
      "[3 rows x 36 columns]\n",
      "\n",
      "df_traveller_vs_sudo 행/열: (400, 36)\n",
      "\n",
      "--- 첫 번째 파일 불러오기 완료 ---\n",
      "\n",
      "======================================================================\n",
      "\n",
      "--- 첫 번째 파일 불러오기 시작 ---\n",
      "'VS_csv.zip' ZIP 파일 내부 목록:\n",
      "- /tn_poi_master_POIMaster.csv\n",
      "- /tn_companion_info_동반자정보_D.csv\n",
      "- /tc_codea_코드A.csv\n",
      "- /tc_codeb_코드B.csv\n",
      "- /tn_activity_his_활동내역_D.csv\n",
      "- /tn_traveller_master_여행객 Master_D.csv\n",
      "- /tn_lodge_consume_his_숙박소비내역_D.csv\n",
      "- /tn_tour_photo_관광사진_D.csv\n",
      "- /tn_adv_consume_his_사전소비내역_D.csv\n",
      "- /tn_mvmn_consume_his_이동수단소비내역_D.csv\n",
      "- /tn_travel_여행_D.csv\n",
      "- /tc_sgg_시군구코드.csv\n",
      "- /tn_move_his_이동내역_D.csv\n",
      "- /tn_activity_consume_his_활동소비내역_D.csv\n",
      "- /tn_visit_area_info_방문지정보_D.csv\n",
      "\n",
      "'/tn_traveller_master_여행객 Master_D.csv' (Validation) 파일이 성공적으로 불러와져 'df_traveller_vs_jeju'로 생성되었습니다.\n",
      "df_traveller_vs_jeju 상위 3행:\n",
      "  TRAVELER_ID  RESIDENCE_SGG_CD GENDER  AGE_GRP  EDU_NM  EDU_FNSH_SE  \\\n",
      "0     a006157                41      남       20       5            1   \n",
      "1     c008192                47      여       20       6            2   \n",
      "2     c007450                41      남       30       7            1   \n",
      "\n",
      "   MARR_STTS  FAMILY_MEMB  JOB_NM  JOB_ETC  ...  TRAVEL_STYL_7  TRAVEL_STYL_8  \\\n",
      "0          1            3     1.0      NaN  ...              5              7   \n",
      "1          1            4    12.0      NaN  ...              1              6   \n",
      "2          1            1    10.0      NaN  ...              4              4   \n",
      "\n",
      "   TRAVEL_STATUS_RESIDENCE  TRAVEL_STATUS_DESTINATION  \\\n",
      "0                      경기도                         경기   \n",
      "1                     경상북도                         충북   \n",
      "2                      경기도                         전북   \n",
      "\n",
      "   TRAVEL_STATUS_ACCOMPANY      TRAVEL_STATUS_YMD  TRAVEL_MOTIVE_1  \\\n",
      "0                   나홀로 여행  2022-10-09~2022-10-10                1   \n",
      "1                   나홀로 여행  2022-10-28~2022-10-29                5   \n",
      "2              2인 여행(가족 외)  2022-10-21~2022-10-22                1   \n",
      "\n",
      "   TRAVEL_MOTIVE_2  TRAVEL_MOTIVE_3  TRAVEL_COMPANIONS_NUM  \n",
      "0              2.0              4.0                      0  \n",
      "1              1.0              7.0                      0  \n",
      "2              7.0              NaN                      1  \n",
      "\n",
      "[3 rows x 36 columns]\n",
      "\n",
      "df_traveller_vs_jeju 행/열: (400, 36)\n",
      "\n",
      "--- 첫 번째 파일 불러오기 완료 ---\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "\n",
    "# --- 1. 첫 번째 파일 불러오기 및 데이터프레임 생성 ---\n",
    "print(\"--- 첫 번째 파일 불러오기 시작 ---\")\n",
    "\n",
    "# 첫 번째 ZIP 파일의 전체 경로\n",
    "# 이 경로가 실제 파일이 있는 경로와 일치하는지 다시 확인해주세요.\n",
    "zip_file_path_vs = r'C:\\Users\\henaj\\Downloads\\278.국내 여행로그 데이터(동부권)\\01-1.정식개방데이터\\Validation\\01.원천데이터\\VS_csv.zip'\n",
    "# ZIP 파일 안에 있는 CSV 파일 이름 (정확한지 다시 확인해주세요. 7-Zip에서 본 이름과 동일해야 합니다.)\n",
    "csv_file_name_in_zip_vs = '/tn_traveller_master_여행객 Master_B.csv'\n",
    "\n",
    "df_traveller_vs_east = None # 변수 초기화\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(zip_file_path_vs):\n",
    "        print(f\"오류: 첫 번째 ZIP 파일이 지정된 경로에 존재하지 않습니다: {zip_file_path_vs}\")\n",
    "    else:\n",
    "        with zipfile.ZipFile(zip_file_path_vs, 'r') as zf_vs:\n",
    "            file_list_in_vs_zip = zf_vs.namelist()\n",
    "            print(f\"'{os.path.basename(zip_file_path_vs)}' ZIP 파일 내부 목록:\")\n",
    "            for name in file_list_in_vs_zip:\n",
    "                print(f\"- {name}\")\n",
    "\n",
    "            if csv_file_name_in_zip_vs in file_list_in_vs_zip:\n",
    "                with zf_vs.open(csv_file_name_in_zip_vs) as csv_file_vs:\n",
    "                    df_traveller_vs_east = pd.read_csv(io.BytesIO(csv_file_vs.read()), encoding='utf-8')\n",
    "                print(f\"\\n'{csv_file_name_in_zip_vs}' (Validation) 파일이 성공적으로 불러와져 'df_traveller_vs_east'로 생성되었습니다.\")\n",
    "                print(f\"df_traveller_vs_east 상위 3행:\\n{df_traveller_vs_east.head(3)}\\n\")\n",
    "                print(f\"df_traveller_vs_east 행/열: {df_traveller_vs_east.shape}\\n\")\n",
    "            else:\n",
    "                print(f\"오류: '{os.path.basename(zip_file_path_vs)}' ZIP 파일 안에 지정된 CSV 파일 ('{csv_file_name_in_zip_vs}')을 찾을 수 없습니다.\")\n",
    "\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"오류: '{zip_file_path_vs}' 파일이 유효한 ZIP 파일이 아니거나 손상되었습니다.\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"오류: '{csv_file_name_in_zip_vs}' 파일을 읽는 중 인코딩 문제가 발생했습니다. (Validation) - {e}\")\n",
    "    print(\"encoding='utf-8' 대신 'cp949' 또는 'euc-kr'을 시도해보세요.\")\n",
    "except Exception as e:\n",
    "    print(f\"첫 번째 파일 처리 중 알 수 없는 오류가 발생했습니다: {e}\")\n",
    "\n",
    "print(\"--- 첫 번째 파일 불러오기 완료 ---\\n\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --- 1. 두 번째 파일 불러오기 및 데이터프레임 생성 ---\n",
    "print(\"--- 두 번째 파일 불러오기 시작 ---\")\n",
    "\n",
    "# 첫 번째 ZIP 파일의 전체 경로\n",
    "# 이 경로가 실제 파일이 있는 경로와 일치하는지 다시 확인해주세요.\n",
    "zip_file_path_vs = r'C:\\Users\\henaj\\Downloads\\279.국내 여행로그 데이터(서부권)\\01-1.정식개방데이터\\Validation\\01.원천데이터\\VS_csv.zip'\n",
    "# ZIP 파일 안에 있는 CSV 파일 이름 (정확한지 다시 확인해주세요. 7-Zip에서 본 이름과 동일해야 합니다.)\n",
    "csv_file_name_in_zip_vs = '/tn_traveller_master_여행객 Master_C.csv'\n",
    "\n",
    "df_traveller_vs_west = None # 변수 초기화\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(zip_file_path_vs):\n",
    "        print(f\"오류: 첫 번째 ZIP 파일이 지정된 경로에 존재하지 않습니다: {zip_file_path_vs}\")\n",
    "    else:\n",
    "        with zipfile.ZipFile(zip_file_path_vs, 'r') as zf_vs:\n",
    "            file_list_in_vs_zip = zf_vs.namelist()\n",
    "            print(f\"'{os.path.basename(zip_file_path_vs)}' ZIP 파일 내부 목록:\")\n",
    "            for name in file_list_in_vs_zip:\n",
    "                print(f\"- {name}\")\n",
    "\n",
    "            if csv_file_name_in_zip_vs in file_list_in_vs_zip:\n",
    "                with zf_vs.open(csv_file_name_in_zip_vs) as csv_file_vs:\n",
    "                    df_traveller_vs_west = pd.read_csv(io.BytesIO(csv_file_vs.read()), encoding='utf-8')\n",
    "                print(f\"\\n'{csv_file_name_in_zip_vs}' (Validation) 파일이 성공적으로 불러와져 'df_traveller_vs_west'로 생성되었습니다.\")\n",
    "                print(f\"df_traveller_vs_west 상위 3행:\\n{df_traveller_vs_west.head(3)}\\n\")\n",
    "                print(f\"df_traveller_vs_west 행/열: {df_traveller_vs_west.shape}\\n\")\n",
    "            else:\n",
    "                print(f\"오류: '{os.path.basename(zip_file_path_vs)}' ZIP 파일 안에 지정된 CSV 파일 ('{csv_file_name_in_zip_vs}')을 찾을 수 없습니다.\")\n",
    "\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"오류: '{zip_file_path_vs}' 파일이 유효한 ZIP 파일이 아니거나 손상되었습니다.\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"오류: '{csv_file_name_in_zip_vs}' 파일을 읽는 중 인코딩 문제가 발생했습니다. (Validation) - {e}\")\n",
    "    print(\"encoding='utf-8' 대신 'cp949' 또는 'euc-kr'을 시도해보세요.\")\n",
    "except Exception as e:\n",
    "    print(f\"첫 번째 파일 처리 중 알 수 없는 오류가 발생했습니다: {e}\")\n",
    "\n",
    "print(\"--- 첫 번째 파일 불러오기 완료 ---\\n\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --- 1. 첫 번째 파일 불러오기 및 데이터프레임 생성 ---\n",
    "print(\"--- 첫 번째 파일 불러오기 시작 ---\")\n",
    "\n",
    "# 첫 번째 ZIP 파일의 전체 경로\n",
    "# 이 경로가 실제 파일이 있는 경로와 일치하는지 다시 확인해주세요.\n",
    "zip_file_path_vs = r'C:\\Users\\henaj\\Downloads\\277.국내 여행로그 데이터(수도권)\\01-1.정식개방데이터\\Validation\\01.원천데이터\\VS_csv.zip'\n",
    "# ZIP 파일 안에 있는 CSV 파일 이름 (정확한지 다시 확인해주세요. 7-Zip에서 본 이름과 동일해야 합니다.)\n",
    "csv_file_name_in_zip_vs = '/tn_traveller_master_여행객 Master_A.csv'\n",
    "\n",
    "df_traveller_vs_sudo = None # 변수 초기화\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(zip_file_path_vs):\n",
    "        print(f\"오류: 첫 번째 ZIP 파일이 지정된 경로에 존재하지 않습니다: {zip_file_path_vs}\")\n",
    "    else:\n",
    "        with zipfile.ZipFile(zip_file_path_vs, 'r') as zf_vs:\n",
    "            file_list_in_vs_zip = zf_vs.namelist()\n",
    "            print(f\"'{os.path.basename(zip_file_path_vs)}' ZIP 파일 내부 목록:\")\n",
    "            for name in file_list_in_vs_zip:\n",
    "                print(f\"- {name}\")\n",
    "\n",
    "            if csv_file_name_in_zip_vs in file_list_in_vs_zip:\n",
    "                with zf_vs.open(csv_file_name_in_zip_vs) as csv_file_vs:\n",
    "                    df_traveller_vs_sudo = pd.read_csv(io.BytesIO(csv_file_vs.read()), encoding='utf-8')\n",
    "                print(f\"\\n'{csv_file_name_in_zip_vs}' (Validation) 파일이 성공적으로 불러와져 'df_traveller_vs_sudo'로 생성되었습니다.\")\n",
    "                print(f\"df_traveller_vs_sudo 상위 3행:\\n{df_traveller_vs_west.head(3)}\\n\")\n",
    "                print(f\"df_traveller_vs_sudo 행/열: {df_traveller_vs_west.shape}\\n\")\n",
    "            else:\n",
    "                print(f\"오류: '{os.path.basename(zip_file_path_vs)}' ZIP 파일 안에 지정된 CSV 파일 ('{csv_file_name_in_zip_vs}')을 찾을 수 없습니다.\")\n",
    "\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"오류: '{zip_file_path_vs}' 파일이 유효한 ZIP 파일이 아니거나 손상되었습니다.\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"오류: '{csv_file_name_in_zip_vs}' 파일을 읽는 중 인코딩 문제가 발생했습니다. (Validation) - {e}\")\n",
    "    print(\"encoding='utf-8' 대신 'cp949' 또는 'euc-kr'을 시도해보세요.\")\n",
    "except Exception as e:\n",
    "    print(f\"첫 번째 파일 처리 중 알 수 없는 오류가 발생했습니다: {e}\")\n",
    "\n",
    "print(\"--- 첫 번째 파일 불러오기 완료 ---\\n\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --- 1. 첫 번째 파일 불러오기 및 데이터프레임 생성 ---\n",
    "print(\"--- 첫 번째 파일 불러오기 시작 ---\")\n",
    "\n",
    "# 첫 번째 ZIP 파일의 전체 경로\n",
    "# 이 경로가 실제 파일이 있는 경로와 일치하는지 다시 확인해주세요.\n",
    "zip_file_path_vs = r'C:\\Users\\henaj\\Downloads\\280.국내 여행로그 데이터(제주도 및 도서지역)\\01-1.정식개방데이터\\Validation\\01.원천데이터\\VS_csv.zip'\n",
    "# ZIP 파일 안에 있는 CSV 파일 이름 (정확한지 다시 확인해주세요. 7-Zip에서 본 이름과 동일해야 합니다.)\n",
    "csv_file_name_in_zip_vs = '/tn_traveller_master_여행객 Master_D.csv'\n",
    "\n",
    "df_traveller_vs_jeju = None # 변수 초기화\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(zip_file_path_vs):\n",
    "        print(f\"오류: 첫 번째 ZIP 파일이 지정된 경로에 존재하지 않습니다: {zip_file_path_vs}\")\n",
    "    else:\n",
    "        with zipfile.ZipFile(zip_file_path_vs, 'r') as zf_vs:\n",
    "            file_list_in_vs_zip = zf_vs.namelist()\n",
    "            print(f\"'{os.path.basename(zip_file_path_vs)}' ZIP 파일 내부 목록:\")\n",
    "            for name in file_list_in_vs_zip:\n",
    "                print(f\"- {name}\")\n",
    "\n",
    "            if csv_file_name_in_zip_vs in file_list_in_vs_zip:\n",
    "                with zf_vs.open(csv_file_name_in_zip_vs) as csv_file_vs:\n",
    "                    df_traveller_vs_jeju = pd.read_csv(io.BytesIO(csv_file_vs.read()), encoding='utf-8')\n",
    "                print(f\"\\n'{csv_file_name_in_zip_vs}' (Validation) 파일이 성공적으로 불러와져 'df_traveller_vs_jeju'로 생성되었습니다.\")\n",
    "                print(f\"df_traveller_vs_jeju 상위 3행:\\n{df_traveller_vs_west.head(3)}\\n\")\n",
    "                print(f\"df_traveller_vs_jeju 행/열: {df_traveller_vs_west.shape}\\n\")\n",
    "            else:\n",
    "                print(f\"오류: '{os.path.basename(zip_file_path_vs)}' ZIP 파일 안에 지정된 CSV 파일 ('{csv_file_name_in_zip_vs}')을 찾을 수 없습니다.\")\n",
    "\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"오류: '{zip_file_path_vs}' 파일이 유효한 ZIP 파일이 아니거나 손상되었습니다.\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"오류: '{csv_file_name_in_zip_vs}' 파일을 읽는 중 인코딩 문제가 발생했습니다. (Validation) - {e}\")\n",
    "    print(\"encoding='utf-8' 대신 'cp949' 또는 'euc-kr'을 시도해보세요.\")\n",
    "except Exception as e:\n",
    "    print(f\"첫 번째 파일 처리 중 알 수 없는 오류가 발생했습니다: {e}\")\n",
    "\n",
    "print(\"--- 첫 번째 파일 불러오기 완료 ---\\n\")\n",
    "print(\"=\"*70 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55c39b12-6807-4910-9cad-4baf5d5de45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 최종 데이터프레임 (df_traveller_vs)의 상위 5개 행:\n",
      "  TRAVELER_ID  RESIDENCE_SGG_CD GENDER  AGE_GRP  EDU_NM  EDU_FNSH_SE  \\\n",
      "0     b000513                41      여       20       6            1   \n",
      "1     b001325                11      여       30       6            1   \n",
      "2     b005349                48      여       20       8            2   \n",
      "3     b009821                43      여       30       6            1   \n",
      "4     b000567                11      여       20       6            1   \n",
      "\n",
      "   MARR_STTS  FAMILY_MEMB  JOB_NM  JOB_ETC  ...  TRAVEL_STYL_7  TRAVEL_STYL_8  \\\n",
      "0          1            1     NaN      3.0  ...              6              7   \n",
      "1          2            2     3.0      NaN  ...              5              4   \n",
      "2          1            3    12.0      NaN  ...              1              3   \n",
      "3          1            1     2.0      NaN  ...              1              6   \n",
      "4          1            5     3.0      NaN  ...              2              6   \n",
      "\n",
      "   TRAVEL_STATUS_RESIDENCE  TRAVEL_STATUS_DESTINATION  \\\n",
      "0                      경기도                         경북   \n",
      "1                    서울특별시                         강원   \n",
      "2                    서울특별시                         경남   \n",
      "3                     충청북도                         경남   \n",
      "4                    서울특별시                         강원   \n",
      "\n",
      "   TRAVEL_STATUS_ACCOMPANY      TRAVEL_STATUS_YMD  TRAVEL_MOTIVE_1  \\\n",
      "0                   나홀로 여행  2022-08-26~2022-08-28                2   \n",
      "1                 2인 가족 여행  2022-08-21~2022-08-22                2   \n",
      "2              2인 여행(가족 외)  2022-10-22~2022-10-23                3   \n",
      "3                   나홀로 여행  2022-10-21~2022-10-22                4   \n",
      "4                 부모 동반 여행  2022-08-15~2022-08-17                2   \n",
      "\n",
      "   TRAVEL_MOTIVE_2  TRAVEL_MOTIVE_3  TRAVEL_COMPANIONS_NUM  \n",
      "0              NaN              NaN                      2  \n",
      "1              NaN              NaN                      1  \n",
      "2              NaN              NaN                      1  \n",
      "3              1.0              2.0                      0  \n",
      "4              1.0              3.0                      4  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "(1600, 36)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 가정: 네 개의 데이터프레임이 이미 로드되어 있다고 가정합니다.\n",
    "# df_traveller_ts_east, df_traveller_ts_west, df_traveller_ts_sudo, df_traveller_ts_jeju\n",
    "\n",
    "# 1. 데이터프레임 합치기 및 전처리\n",
    "# pd.concat()을 사용하여 데이터프레임을 합칩니다.\n",
    "# 첫 번째 데이터프레임(df_traveller_ts_east)의 열 이름을 기준으로 나머지 데이터프레임을 합치면\n",
    "# 열 이름 중복 문제를 피할 수 있습니다.\n",
    "# ignore_index=True를 설정하여 합쳐진 데이터프레임의 인덱스가 0부터 새로 시작하도록 합니다.\n",
    "df_traveller_vs = pd.concat([df_traveller_vs_east, df_traveller_vs_west, df_traveller_vs_sudo, df_traveller_vs_jeju], ignore_index=True)\n",
    "\n",
    "# 2. 결측치 처리 (특정 열에 결측치가 하나라도 있는 행 삭제)\n",
    "# dropna() 함수를 사용하여 특정 열(subset)에 NaN 값이 있는 행을 삭제합니다.\n",
    "# inplace=True를 설정하여 원래 데이터프레임에 변경사항을 바로 적용합니다.\n",
    "columns_to_check = ['TRAVEL_STATUS_ACCOMPANY', 'TRAVEL_COMPANIONS_NUM', 'INCOME', 'TRAVEL_STYL_1', 'TRAVEL_MOTIVE_1', 'TRAVEL_STATUS_DESTINATION']\n",
    "df_traveller_ts.dropna(subset=columns_to_check, inplace=True)\n",
    "\n",
    "# 3. 특정 열의 값 범위에 맞지 않는 행 삭제\n",
    "# 각 열에 대해 주어진 범위에 해당하는지 확인하고, 해당하지 않는 행을 삭제합니다.\n",
    "# ~를 사용하여 조건을 만족하지 않는 행을 선택하고, df.index를 통해 해당 행의 인덱스를 삭제합니다.\n",
    "# isin() 메서드는 주어진 리스트에 값이 포함되어 있는지 확인하는 데 유용합니다.\n",
    "\n",
    "# INCOME 열 처리 (1~12 정수)\n",
    "income_range = list(range(1, 13))\n",
    "df_traveller_vs = df_traveller_vs[df_traveller_vs['INCOME'].isin(income_range)]\n",
    "\n",
    "# TRAVEL_STYL_1 열 처리 (1~7 정수)\n",
    "travel_style_range = list(range(1, 8))\n",
    "df_traveller_vs = df_traveller_vs[df_traveller_vs['TRAVEL_STYL_1'].isin(travel_style_range)]\n",
    "\n",
    "# TRAVEL_MOTIVE_1 열 처리 (1~10 정수)\n",
    "travel_motive_range = list(range(1, 11))\n",
    "df_traveller_vs = df_traveller_vs[df_traveller_vs['TRAVEL_MOTIVE_1'].isin(travel_motive_range)]\n",
    "\n",
    "# 모든 작업이 완료된 후 데이터프레임의 첫 5행을 출력하여 확인\n",
    "print(\"전처리 후 최종 데이터프레임 (df_traveller_vs)의 상위 5개 행:\")\n",
    "print(df_traveller_vs.head())\n",
    "print(df_traveller_vs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b86d725-1473-4ca2-9781-0bf180b654d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_traveller_ts shape: (12799, 36)\n",
      "df_traveller_vs shape: (1600, 36)\n",
      "X_tr.shape, X_val.shape, X_test.shape: (11519, 5) (1280, 5) (1600, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['INCOME', 'TRAVEL_MOTIVE_1', 'TRAVEL_STATUS_ACCOMPANY', 'TRAVEL_STYL_1']\n",
      "  _log_warning('categorical_feature in Dataset is overridden.\\n'\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\ttraining's multi_logloss: 2.27556\tvalid_1's multi_logloss: 2.54216\n",
      "base model best_iteration_:  14\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['INCOME', 'TRAVEL_MOTIVE_1', 'TRAVEL_STATUS_ACCOMPANY', 'TRAVEL_STYL_1']\n",
      "  _log_warning('categorical_feature in Dataset is overridden.\\n'\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch best params: {'colsample_bytree': 0.8, 'max_depth': 10, 'min_child_samples': 30, 'num_leaves': 31, 'subsample': 0.8}\n",
      "GridSearch best score (CV): 0.20609407108332609\n",
      "best_model n_estimators (best_iteration_): 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 최종 Test 평가 (get_clf_eval) ===\n",
      "정확도(accuracy): 0.2062\n",
      "정밀도(precision, weighted): 0.0758\n",
      "재현율(recall, weighted): 0.2062\n",
      "F1 score (weighted): 0.1032\n",
      "\n",
      "Top feature importances:\n",
      "                    feature  importance_gain\n",
      "0    TRAVEL_COMPANIONS_NUM      6745.795279\n",
      "1          TRAVEL_MOTIVE_1      4463.538562\n",
      "2            TRAVEL_STYL_1      4032.992544\n",
      "3                   INCOME      3798.633405\n",
      "4  TRAVEL_STATUS_ACCOMPANY      3478.578580\n",
      "Feature importance figure saved to: C:\\Users\\henaj\\Downloads\\278.국내 여행로그 데이터(동부권)\\01-1.정식개방데이터\\Training\\01.원천데이터\\feature_importance.png\n",
      "Results exported to Excel: C:\\Users\\henaj\\Downloads\\278.국내 여행로그 데이터(동부권)\\01-1.정식개방데이터\\Training\\01.원천데이터\\lightGBM-modelling.xlsx\n",
      "Label encoders saved to: C:\\Users\\henaj\\Downloads\\278.국내 여행로그 데이터(동부권)\\01-1.정식개방데이터\\Training\\01.원천데이터\\label_encoders.pkl\n",
      "모델링 파이프라인 완료.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from lightgbm import LGBMClassifier, plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ====== 0. 사용자 환경 설정 / 파일 저장 경로 ======\n",
    "# 엑셀 저장 경로(요청하신 경로 & 파일명)\n",
    "save_dir = r\"C:\\Users\\henaj\\Downloads\\278.국내 여행로그 데이터(동부권)\\01-1.정식개방데이터\\Training\\01.원천데이터\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "excel_path = os.path.join(save_dir, \"lightGBM-modelling.xlsx\")\n",
    "\n",
    "# ====== 1. 평가 함수 (요청하신 get_clf_eval) ======\n",
    "def get_clf_eval(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    print(f\"정확도(accuracy): {acc:.4f}\")\n",
    "    print(f\"정밀도(precision, weighted): {precision:.4f}\")\n",
    "    print(f\"재현율(recall, weighted): {recall:.4f}\")\n",
    "    print(f\"F1 score (weighted): {f1:.4f}\")\n",
    "    # return a dict for saving to excel if needed\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# ====== 2. 사용할 컬럼 정의 ======\n",
    "features = ['TRAVEL_COMPANIONS_NUM', 'TRAVEL_STATUS_ACCOMPANY', 'INCOME', 'TRAVEL_STYL_1', 'TRAVEL_MOTIVE_1']\n",
    "target_col = 'TRAVEL_STATUS_DESTINATION'\n",
    "\n",
    "# ====== 3. 데이터 (사용자 환경에 이미 로드된 데이터프레임 사용) ======\n",
    "# 사용자 말에 따르면 df_traveller_ts (12800) 과 df_traveller_vs (1600)가 이미 존재\n",
    "# df_traveller_ts, df_traveller_vs 가 현재 네임스페이스에 있다고 가정\n",
    "# (만약 파일에서 불러와야 하면 pd.read_csv 등으로 불러오세요)\n",
    "\n",
    "# 간단한 확인 (필요시 주석처리)\n",
    "print(\"df_traveller_ts shape:\", df_traveller_ts.shape)\n",
    "print(\"df_traveller_vs shape:\", df_traveller_vs.shape)\n",
    "\n",
    "# ====== 4. 라벨 인코더 준비 (범주형 feature + target) ======\n",
    "# 범주형 features 목록 (문자열 카테고리인 것들)\n",
    "categorical_features = ['TRAVEL_STATUS_ACCOMPANY', 'INCOME', 'TRAVEL_STYL_1', 'TRAVEL_MOTIVE_1']\n",
    "\n",
    "# 복사본 사용 (원본 변경을 피하기 위해)\n",
    "train_df = df_traveller_ts.copy()\n",
    "test_df  = df_traveller_vs.copy()\n",
    "\n",
    "# Instantiate LabelEncoders 저장용 딕셔너리 (나중에 inverse_transform 가능)\n",
    "encoders = {}\n",
    "\n",
    "# 4-1. feature들의 인코딩 (문자열 카테고리 -> 정수)\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    # fit on combined values (train + test) to ensure same encoding across sets (안정성)\n",
    "    combined_vals = pd.concat([train_df[col].astype(str), test_df[col].astype(str)], axis=0)\n",
    "    le.fit(combined_vals)\n",
    "    train_df[col] = le.transform(train_df[col].astype(str))\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "# 4-2. target (여행목적지) 라벨 인코딩\n",
    "le_target = LabelEncoder()\n",
    "combined_target = pd.concat([train_df[target_col].astype(str), test_df[target_col].astype(str)], axis=0)\n",
    "le_target.fit(combined_target)\n",
    "train_df[target_col] = le_target.transform(train_df[target_col].astype(str))\n",
    "test_df[target_col] = le_target.transform(test_df[target_col].astype(str))\n",
    "encoders['target'] = le_target\n",
    "\n",
    "# ====== 5. X / y 분리 및 내부 90:10 split (train-> X_tr/X_val, y_tr/y_val) ======\n",
    "X_full = train_df[features]\n",
    "y_full = train_df[target_col]\n",
    "\n",
    "# 내부 validation (train 데이터에서 10% 떼기), stratify 사용 권장 (클래스 비율 유지)\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_full, y_full, test_size=0.10, random_state=156, stratify=y_full\n",
    ")\n",
    "\n",
    "# 외부 최종 테스트 (제공된 validation 데이터)\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "print(\"X_tr.shape, X_val.shape, X_test.shape:\", X_tr.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "# ====== 6. LightGBM 기본 설정 및 early stopping 으로 학습 (초기 실행) ======\n",
    "num_classes = len(le_target.classes_)\n",
    "base_lgbm = LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=num_classes,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=2000,  # 크게 잡아두고 early stopping으로 멈출 것\n",
    "    random_state=156,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# fit 기본 한 번 실행 (early stopping 확인용) - verbose는 50 간격으로 로그 출력\n",
    "base_lgbm.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_tr, y_tr), (X_val, y_val)],\n",
    "    eval_metric='multi_logloss',   # 다중 클래스의 경우 multi_logloss 권장\n",
    "    early_stopping_rounds=50,\n",
    "    categorical_feature=categorical_features,\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "print(\"base model best_iteration_: \", getattr(base_lgbm, \"best_iteration_\", None))\n",
    "\n",
    "# ====== 7. 하이퍼파라미터 튜닝 (GridSearchCV) ======\n",
    "# 그리드에 넣을 파라미터 범위(예시는 합리적 범위; 필요하면 조정하세요)\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50],\n",
    "    'max_depth': [-1, 10, 20],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# GridSearchCV 객체 (교차검증 3-fold 사용)\n",
    "grid = GridSearchCV(\n",
    "    estimator=base_lgbm,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    refit=True  # best_estimator_로 재학습(이 옵션 True면 fit 끝난 뒤 best_estimator_가 준비됨)\n",
    ")\n",
    "\n",
    "# GridSearchCV.fit에 추가로 전달할 fit params (LightGBM의 early stopping 등)\n",
    "fit_params = {\n",
    "    \"eval_set\": [(X_tr, y_tr), (X_val, y_val)],\n",
    "    \"eval_metric\": \"multi_logloss\",\n",
    "    \"early_stopping_rounds\": 50,\n",
    "    \"categorical_feature\": categorical_features,\n",
    "    \"verbose\": False\n",
    "}\n",
    "\n",
    "# 주의: sklearn 버전에 따라 GridSearchCV.fit(...)가 추가 fit params를 허용하지 않을 수 있음.\n",
    "# 허용하지 않는 환경이라면, GridSearchCV 없이 수동으로 하이퍼파라미터 루프를 돌려야 합니다.\n",
    "grid.fit(X_tr, y_tr, **fit_params)\n",
    "\n",
    "print(\"GridSearch best params:\", grid.best_params_)\n",
    "print(\"GridSearch best score (CV):\", grid.best_score_)\n",
    "\n",
    "# 최적 모델\n",
    "best_model = grid.best_estimator_\n",
    "print(\"best_model n_estimators (best_iteration_):\", getattr(best_model, \"best_iteration_\", None))\n",
    "\n",
    "# ====== 8. 최종 모델 준비 (전체 training 데이터로 재학습) ======\n",
    "# 보통 내부 validation에서 찾은 best_iteration_를 이용해 전체 train 데이터로 재학습하면 좋음\n",
    "best_iter = getattr(best_model, \"best_iteration_\", None)\n",
    "if best_iter is None:\n",
    "    # best_iteration_가 없다면 best_model의 n_estimators 사용\n",
    "    best_iter = best_model.get_params().get(\"n_estimators\", 200)\n",
    "\n",
    "# 새 모델을 best params로 재생성하여 전체 training(12800)으로 학습\n",
    "final_params = best_model.get_params().copy()\n",
    "# set n_estimators = best_iter for final training\n",
    "final_params['n_estimators'] = best_iter\n",
    "\n",
    "final_model = LGBMClassifier(**final_params)\n",
    "final_model.fit(\n",
    "    X_full, y_full,\n",
    "    categorical_feature=categorical_features,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# ====== 9. 최종 평가 (제공된 validation 데이터에 대해) ======\n",
    "y_pred_test = final_model.predict(X_test)\n",
    "# y_proba_test = final_model.predict_proba(X_test)  # 필요하면 사용\n",
    "\n",
    "print(\"\\n=== 최종 Test 평가 (get_clf_eval) ===\")\n",
    "metrics_dict = get_clf_eval(y_test, y_pred_test)\n",
    "\n",
    "# ====== 10. Feature importance 시각화 및 DataFrame 생성 ======\n",
    "# 중요도 추출 (gain 기준)\n",
    "try:\n",
    "    importances = final_model.booster_.feature_importance(importance_type='gain')\n",
    "    feature_names = final_model.booster_.feature_name()\n",
    "    fi_df = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance_gain\": importances\n",
    "    }).sort_values(by='importance_gain', ascending=False).reset_index(drop=True)\n",
    "except Exception as e:\n",
    "    # fallback: sklearn API로\n",
    "    fi_df = pd.DataFrame({\n",
    "        \"feature\": final_model.feature_name_,\n",
    "        \"importance\": final_model.feature_importances_\n",
    "    }).sort_values(by='importance', ascending=False).reset_index(drop=True)\n",
    "    fi_df.rename(columns={\"importance\": \"importance_gain\"}, inplace=True)\n",
    "\n",
    "print(\"\\nTop feature importances:\\n\", fi_df.head(10))\n",
    "\n",
    "# Plot importance (그래프 저장)\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = plot_importance(final_model, max_num_features=10, importance_type='gain')\n",
    "plt.title(\"LightGBM Feature Importance (gain)\")\n",
    "plt.tight_layout()\n",
    "img_path = os.path.join(save_dir, \"feature_importance.png\")\n",
    "plt.savefig(img_path, dpi=150)\n",
    "plt.close()\n",
    "print(\"Feature importance figure saved to:\", img_path)\n",
    "\n",
    "# ====== 11. 평가 결과 및 중요도 엑셀로 저장 ======\n",
    "# metrics -> DataFrame\n",
    "metrics_df = pd.DataFrame([metrics_dict], index=[\"final_test_eval\"])\n",
    "\n",
    "# confusion matrix (저장용)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index=[f\"true_{c}\" for c in le_target.classes_],\n",
    "                     columns=[f\"pred_{c}\" for c in le_target.classes_])\n",
    "\n",
    "# Save to excel with multiple sheets\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl', mode='w') as writer:\n",
    "    metrics_df.to_excel(writer, sheet_name='metrics', index=True)\n",
    "    fi_df.to_excel(writer, sheet_name='feature_importance', index=False)\n",
    "    cm_df.to_excel(writer, sheet_name='confusion_matrix')\n",
    "    # also save encoding maps (optional) - save target mapping\n",
    "    target_map = pd.DataFrame({\n",
    "        \"label\": list(le_target.classes_),\n",
    "        \"encoded\": list(range(len(le_target.classes_)))\n",
    "    })\n",
    "    target_map.to_excel(writer, sheet_name='target_label_map', index=False)\n",
    "\n",
    "print(\"Results exported to Excel:\", excel_path)\n",
    "\n",
    "# ====== 12. (선택) 인코더 저장 – 나중에 예측 결과를 원래 라벨로 복원하고 싶다면 사용 ======\n",
    "import pickle\n",
    "enc_path = os.path.join(save_dir, \"label_encoders.pkl\")\n",
    "with open(enc_path, \"wb\") as f:\n",
    "    pickle.dump(encoders, f)\n",
    "print(\"Label encoders saved to:\", enc_path)\n",
    "\n",
    "# 끝\n",
    "print(\"모델링 파이프라인 완료.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6201c17e-c598-41d9-96cf-8edae93620e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_traveller_ts shape: (12799, 36)\n",
      "df_traveller_vs shape: (1600, 36)\n",
      "X_train:  X_train.shape X_test: X_test.shape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "For early stopping, at least one dataset and eval metric is required for evaluation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 95\u001b[0m\n\u001b[0;32m     85\u001b[0m base_lgbm \u001b[38;5;241m=\u001b[39m LGBMClassifier(\n\u001b[0;32m     86\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     87\u001b[0m     num_class\u001b[38;5;241m=\u001b[39mnum_classes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     91\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     92\u001b[0m )\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# fit 기본 한 번 실행 (early stopping 확인용) - verbose는 50 간격으로 로그 출력\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m base_lgbm\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     96\u001b[0m     X_train, y_train,\n\u001b[0;32m     97\u001b[0m     eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulti_logloss\u001b[39m\u001b[38;5;124m'\u001b[39m,   \u001b[38;5;66;03m# 다중 클래스의 경우 multi_logloss 권장\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     99\u001b[0m     categorical_feature\u001b[38;5;241m=\u001b[39mcategorical_features,\n\u001b[0;32m    100\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[0;32m    101\u001b[0m )\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase model best_iteration_: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mgetattr\u001b[39m(base_lgbm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_iteration_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# ====== 7. 하이퍼파라미터 튜닝 (GridSearchCV) ======\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# 그리드에 넣을 파라미터 범위(예시는 합리적 범위; 필요하면 조정하세요)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:967\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m             valid_sets[i] \u001b[38;5;241m=\u001b[39m (valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y))\n\u001b[1;32m--> 967\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, _y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, init_score\u001b[38;5;241m=\u001b[39minit_score, eval_set\u001b[38;5;241m=\u001b[39mvalid_sets,\n\u001b[0;32m    968\u001b[0m             eval_names\u001b[38;5;241m=\u001b[39meval_names, eval_sample_weight\u001b[38;5;241m=\u001b[39meval_sample_weight,\n\u001b[0;32m    969\u001b[0m             eval_class_weight\u001b[38;5;241m=\u001b[39meval_class_weight, eval_init_score\u001b[38;5;241m=\u001b[39meval_init_score,\n\u001b[0;32m    970\u001b[0m             eval_metric\u001b[38;5;241m=\u001b[39meval_metric, early_stopping_rounds\u001b[38;5;241m=\u001b[39mearly_stopping_rounds,\n\u001b[0;32m    971\u001b[0m             verbose\u001b[38;5;241m=\u001b[39mverbose, feature_name\u001b[38;5;241m=\u001b[39mfeature_name, categorical_feature\u001b[38;5;241m=\u001b[39mcategorical_feature,\n\u001b[0;32m    972\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mcallbacks, init_model\u001b[38;5;241m=\u001b[39minit_model)\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:748\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    745\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    746\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m--> 748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m    749\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    750\u001b[0m     train_set\u001b[38;5;241m=\u001b[39mtrain_set,\n\u001b[0;32m    751\u001b[0m     num_boost_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators,\n\u001b[0;32m    752\u001b[0m     valid_sets\u001b[38;5;241m=\u001b[39mvalid_sets,\n\u001b[0;32m    753\u001b[0m     valid_names\u001b[38;5;241m=\u001b[39meval_names,\n\u001b[0;32m    754\u001b[0m     fobj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fobj,\n\u001b[0;32m    755\u001b[0m     feval\u001b[38;5;241m=\u001b[39meval_metrics_callable,\n\u001b[0;32m    756\u001b[0m     init_model\u001b[38;5;241m=\u001b[39minit_model,\n\u001b[0;32m    757\u001b[0m     feature_name\u001b[38;5;241m=\u001b[39mfeature_name,\n\u001b[0;32m    758\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks\n\u001b[0;32m    759\u001b[0m )\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evals_result:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lightgbm\\engine.py:302\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_after_iter:\n\u001b[1;32m--> 302\u001b[0m         cb(callback\u001b[38;5;241m.\u001b[39mCallbackEnv(model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[0;32m    303\u001b[0m                                 params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    304\u001b[0m                                 iteration\u001b[38;5;241m=\u001b[39mi,\n\u001b[0;32m    305\u001b[0m                                 begin_iteration\u001b[38;5;241m=\u001b[39minit_iteration,\n\u001b[0;32m    306\u001b[0m                                 end_iteration\u001b[38;5;241m=\u001b[39minit_iteration \u001b[38;5;241m+\u001b[39m num_boost_round,\n\u001b[0;32m    307\u001b[0m                                 evaluation_result_list\u001b[38;5;241m=\u001b[39mevaluation_result_list))\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m callback\u001b[38;5;241m.\u001b[39mEarlyStopException \u001b[38;5;28;01mas\u001b[39;00m earlyStopException:\n\u001b[0;32m    309\u001b[0m     booster\u001b[38;5;241m.\u001b[39mbest_iteration \u001b[38;5;241m=\u001b[39m earlyStopException\u001b[38;5;241m.\u001b[39mbest_iteration \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lightgbm\\callback.py:256\u001b[0m, in \u001b[0;36mearly_stopping.<locals>._callback\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_callback\u001b[39m(env: CallbackEnv) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cmp_op:\n\u001b[1;32m--> 256\u001b[0m         _init(env)\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m enabled[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lightgbm\\callback.py:226\u001b[0m, in \u001b[0;36mearly_stopping.<locals>._init\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env\u001b[38;5;241m.\u001b[39mevaluation_result_list:\n\u001b[1;32m--> 226\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFor early stopping, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    227\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat least one dataset and eval metric is required for evaluation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m    230\u001b[0m     _log_info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining until validation scores don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt improve for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstopping_rounds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: For early stopping, at least one dataset and eval metric is required for evaluation"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from lightgbm import LGBMClassifier, plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ====== 0. 사용자 환경 설정 / 파일 저장 경로 ======\n",
    "# 엑셀 저장 경로(요청하신 경로 & 파일명)\n",
    "save_dir = r\"C:\\Users\\henaj\\Downloads\\278.국내 여행로그 데이터(동부권)\\01-1.정식개방데이터\\Training\\01.원천데이터\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "excel_path = os.path.join(save_dir, \"lightGBM-modelling2.xlsx\")\n",
    "\n",
    "# ====== 1. 평가 함수 (요청하신 get_clf_eval) ======\n",
    "def get_clf_eval(y_test, y_pred):\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    print(f\"정확도(accuracy): {acc:.4f}\")\n",
    "    print(f\"정밀도(precision, weighted): {precision:.4f}\")\n",
    "    print(f\"재현율(recall, weighted): {recall:.4f}\")\n",
    "    print(f\"F1 score (weighted): {f1:.4f}\")\n",
    "    # return a dict for saving to excel if needed\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# ====== 2. 사용할 컬럼 정의 ======\n",
    "features = ['TRAVEL_COMPANIONS_NUM', 'TRAVEL_STATUS_ACCOMPANY', 'INCOME', 'TRAVEL_STYL_1', 'TRAVEL_MOTIVE_1']\n",
    "target_col = 'TRAVEL_STATUS_DESTINATION'\n",
    "\n",
    "# ====== 3. 데이터 (사용자 환경에 이미 로드된 데이터프레임 사용) ======\n",
    "# 사용자 말에 따르면 df_traveller_ts (12800) 과 df_traveller_vs (1600)가 이미 존재\n",
    "# df_traveller_ts, df_traveller_vs 가 현재 네임스페이스에 있다고 가정\n",
    "# (만약 파일에서 불러와야 하면 pd.read_csv 등으로 불러오세요)\n",
    "\n",
    "# 간단한 확인 (필요시 주석처리)\n",
    "print(\"df_traveller_ts shape:\", df_traveller_ts.shape)\n",
    "print(\"df_traveller_vs shape:\", df_traveller_vs.shape)\n",
    "\n",
    "# ====== 4. 라벨 인코더 준비 (범주형 feature + target) ======\n",
    "# 범주형 features 목록 (문자열 카테고리인 것들)\n",
    "categorical_features = ['TRAVEL_STATUS_ACCOMPANY', 'INCOME', 'TRAVEL_STYL_1', 'TRAVEL_MOTIVE_1']\n",
    "\n",
    "# 복사본 사용 (원본 변경을 피하기 위해)\n",
    "train_df = df_traveller_ts.copy()\n",
    "test_df  = df_traveller_vs.copy()\n",
    "\n",
    "# Instantiate LabelEncoders 저장용 딕셔너리 (나중에 inverse_transform 가능)\n",
    "encoders = {}\n",
    "\n",
    "# 4-1. feature들의 인코딩 (문자열 카테고리 -> 정수)\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    # fit on combined values (train + test) to ensure same encoding across sets (안정성)\n",
    "    combined_vals = pd.concat([train_df[col].astype(str), test_df[col].astype(str)], axis=0)\n",
    "    le.fit(combined_vals)\n",
    "    train_df[col] = le.transform(train_df[col].astype(str))\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "# 4-2. target (여행목적지) 라벨 인코딩\n",
    "le_target = LabelEncoder()\n",
    "combined_target = pd.concat([train_df[target_col].astype(str), test_df[target_col].astype(str)], axis=0)\n",
    "le_target.fit(combined_target)\n",
    "train_df[target_col] = le_target.transform(train_df[target_col].astype(str))\n",
    "test_df[target_col] = le_target.transform(test_df[target_col].astype(str))\n",
    "encoders['target'] = le_target\n",
    "\n",
    "# ====== 5. X / y 분리 ======\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[target_col]\n",
    "\n",
    "\n",
    "# 외부 최종 테스트 (제공된 validation 데이터)\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "print(\"X_train:\",\" X_train.shape\", \"X_test:\", \"X_test.shape\")\n",
    "\n",
    "# ====== 6. LightGBM 기본 설정 및 early stopping 으로 학습 (초기 실행) ======\n",
    "num_classes = len(le_target.classes_)\n",
    "base_lgbm = LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=num_classes,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=2000,  # 크게 잡아두고 early stopping으로 멈출 것\n",
    "    random_state=156,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# fit 기본 한 번 실행 (early stopping 확인용) - verbose는 50 간격으로 로그 출력\n",
    "base_lgbm.fit(\n",
    "    X_train, y_train,\n",
    "    eval_metric='multi_logloss',   # 다중 클래스의 경우 multi_logloss 권장\n",
    "    early_stopping_rounds=50,\n",
    "    categorical_feature=categorical_features,\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "print(\"base model best_iteration_: \", getattr(base_lgbm, \"best_iteration_\", None))\n",
    "\n",
    "# ====== 7. 하이퍼파라미터 튜닝 (GridSearchCV) ======\n",
    "# 그리드에 넣을 파라미터 범위(예시는 합리적 범위; 필요하면 조정하세요)\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50],\n",
    "    'max_depth': [-1, 10, 20],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# GridSearchCV 객체 (교차검증 10-fold 사용)\n",
    "grid = GridSearchCV(\n",
    "    estimator=base_lgbm,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    cv=10,\n",
    "    verbose=2,\n",
    "    refit=True  # best_estimator_로 재학습(이 옵션 True면 fit 끝난 뒤 best_estimator_가 준비됨)\n",
    ")\n",
    "\n",
    "# GridSearchCV.fit에 추가로 전달할 fit params (LightGBM의 early stopping 등)\n",
    "fit_params = {\n",
    "    \n",
    "    \"eval_metric\": \"multi_logloss\",\n",
    "    \"early_stopping_rounds\": 50,\n",
    "    \"categorical_feature\": categorical_features,\n",
    "    \"verbose\": False\n",
    "}\n",
    "\n",
    "# 주의: sklearn 버전에 따라 GridSearchCV.fit(...)가 추가 fit params를 허용하지 않을 수 있음.\n",
    "# 허용하지 않는 환경이라면, GridSearchCV 없이 수동으로 하이퍼파라미터 루프를 돌려야 합니다.\n",
    "grid.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "print(\"GridSearch best params:\", grid.best_params_)\n",
    "print(\"GridSearch best score (CV):\", grid.best_score_)\n",
    "\n",
    "# 최적 모델\n",
    "best_model = grid.best_estimator_\n",
    "print(\"best_model n_estimators (best_iteration_):\", getattr(best_model, \"best_iteration_\", None))\n",
    "\n",
    "# ====== 8. 최종 모델 준비 (전체 training 데이터로 재학습) ======\n",
    "# 보통 내부 validation에서 찾은 best_iteration_를 이용해 전체 train 데이터로 재학습하면 좋음\n",
    "best_iter = getattr(best_model, \"best_iteration_\", None)\n",
    "if best_iter is None:\n",
    "    # best_iteration_가 없다면 best_model의 n_estimators 사용\n",
    "    best_iter = best_model.get_params().get(\"n_estimators\", 200)\n",
    "\n",
    "# 새 모델을 best params로 재생성하여 전체 training(12800)으로 학습\n",
    "final_params = best_model.get_params().copy()\n",
    "# set n_estimators = best_iter for final training\n",
    "final_params['n_estimators'] = best_iter\n",
    "\n",
    "final_model = LGBMClassifier(**final_params)\n",
    "final_model.fit(\n",
    "    X_train, y_train,\n",
    "    categorical_feature=categorical_features,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# ====== 9. 최종 평가 (제공된 validation 데이터에 대해) ======\n",
    "y_pred_test = final_model.predict(X_test)\n",
    "# y_proba_test = final_model.predict_proba(X_test)  # 필요하면 사용\n",
    "\n",
    "print(\"\\n=== 최종 Test 평가 (get_clf_eval) ===\")\n",
    "metrics_dict = get_clf_eval(y_test, y_pred_test)\n",
    "\n",
    "# ====== 10. Feature importance 시각화 및 DataFrame 생성 ======\n",
    "# 중요도 추출 (gain 기준)\n",
    "try:\n",
    "    importances = final_model.booster_.feature_importance(importance_type='gain')\n",
    "    feature_names = final_model.booster_.feature_name()\n",
    "    fi_df = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance_gain\": importances\n",
    "    }).sort_values(by='importance_gain', ascending=False).reset_index(drop=True)\n",
    "except Exception as e:\n",
    "    # fallback: sklearn API로\n",
    "    fi_df = pd.DataFrame({\n",
    "        \"feature\": final_model.feature_name_,\n",
    "        \"importance\": final_model.feature_importances_\n",
    "    }).sort_values(by='importance', ascending=False).reset_index(drop=True)\n",
    "    fi_df.rename(columns={\"importance\": \"importance_gain\"}, inplace=True)\n",
    "\n",
    "print(\"\\nTop feature importances:\\n\", fi_df.head(10))\n",
    "\n",
    "# Plot importance (그래프 저장)\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = plot_importance(final_model, max_num_features=10, importance_type='gain')\n",
    "plt.title(\"LightGBM Feature Importance (gain)\")\n",
    "plt.tight_layout()\n",
    "img_path = os.path.join(save_dir, \"feature_importance.png\")\n",
    "plt.savefig(img_path, dpi=150)\n",
    "plt.close()\n",
    "print(\"Feature importance figure saved to:\", img_path)\n",
    "\n",
    "# ====== 11. 평가 결과 및 중요도 엑셀로 저장 ======\n",
    "# metrics -> DataFrame\n",
    "metrics_df = pd.DataFrame([metrics_dict], index=[\"final_test_eval\"])\n",
    "\n",
    "# confusion matrix (저장용)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index=[f\"true_{c}\" for c in le_target.classes_],\n",
    "                     columns=[f\"pred_{c}\" for c in le_target.classes_])\n",
    "\n",
    "# Save to excel with multiple sheets\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl', mode='w') as writer:\n",
    "    metrics_df.to_excel(writer, sheet_name='metrics', index=True)\n",
    "    fi_df.to_excel(writer, sheet_name='feature_importance', index=False)\n",
    "    cm_df.to_excel(writer, sheet_name='confusion_matrix')\n",
    "    # also save encoding maps (optional) - save target mapping\n",
    "    target_map = pd.DataFrame({\n",
    "        \"label\": list(le_target.classes_),\n",
    "        \"encoded\": list(range(len(le_target.classes_)))\n",
    "    })\n",
    "    target_map.to_excel(writer, sheet_name='target_label_map', index=False)\n",
    "\n",
    "print(\"Results exported to Excel:\", excel_path)\n",
    "\n",
    "# ====== 12. (선택) 인코더 저장 – 나중에 예측 결과를 원래 라벨로 복원하고 싶다면 사용 ======\n",
    "import pickle\n",
    "enc_path = os.path.join(save_dir, \"label_encoders.pkl2\")\n",
    "with open(enc_path, \"wb\") as f:\n",
    "    pickle.dump(encoders, f)\n",
    "print(\"Label encoders saved to:\", enc_path)\n",
    "\n",
    "# 끝\n",
    "print(\"모델링 파이프라인 완료.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c6596ed-47ff-4a20-8392-f522bf49a42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_traveller_ts shape: (12799, 36)\n",
      "df_traveller_vs shape: (1600, 36)\n",
      "X_tr.shape, X_val.shape, X_test.shape: (11519, 5) (1280, 5) (1600, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['INCOME', 'TRAVEL_MOTIVE_1', 'TRAVEL_STATUS_ACCOMPANY', 'TRAVEL_STYL_1']\n",
      "  _log_warning('categorical_feature in Dataset is overridden.\\n'\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\ttraining's multi_logloss: 2.27556\tvalid_1's multi_logloss: 2.54216\n",
      "base model best_iteration_:  14\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['INCOME', 'TRAVEL_MOTIVE_1', 'TRAVEL_STATUS_ACCOMPANY', 'TRAVEL_STYL_1']\n",
      "  _log_warning('categorical_feature in Dataset is overridden.\\n'\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch best params: {'colsample_bytree': 0.8, 'max_depth': -1, 'min_child_samples': 20, 'num_leaves': 31, 'subsample': 0.8}\n",
      "GridSearch best score (CV): 0.20982727881552274\n",
      "best_model n_estimators (best_iteration_): 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\henaj\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 최종 Test 평가 (get_clf_eval) ===\n",
      "정확도(accuracy): 0.2000\n",
      "정밀도(precision, weighted): 0.0718\n",
      "재현율(recall, weighted): 0.2000\n",
      "F1 score (weighted): 0.1003\n",
      "\n",
      "Top feature importances:\n",
      "                    feature  importance_gain\n",
      "0    TRAVEL_COMPANIONS_NUM      8725.417806\n",
      "1          TRAVEL_MOTIVE_1      5017.341599\n",
      "2            TRAVEL_STYL_1      4661.766460\n",
      "3                   INCOME      4430.007880\n",
      "4  TRAVEL_STATUS_ACCOMPANY      3983.285307\n",
      "Feature importance figure saved to: C:\\Users\\henaj\\Downloads\\278.국내 여행로그 데이터(동부권)\\01-1.정식개방데이터\\Training\\01.원천데이터\\feature_importance.png\n",
      "Results exported to Excel: C:\\Users\\henaj\\Downloads\\278.국내 여행로그 데이터(동부권)\\01-1.정식개방데이터\\Training\\01.원천데이터\\lightGBM-modelling2.xlsx\n",
      "Label encoders saved to: C:\\Users\\henaj\\Downloads\\278.국내 여행로그 데이터(동부권)\\01-1.정식개방데이터\\Training\\01.원천데이터\\label_encoders.pkl2\n",
      "모델링 파이프라인 완료.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*---처음 했던 것 ten-fold 교차 검증으로 다시 바꿔서 진행----\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from lightgbm import LGBMClassifier, plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ====== 0. 사용자 환경 설정 / 파일 저장 경로 ======\n",
    "# 엑셀 저장 경로(요청하신 경로 & 파일명)\n",
    "save_dir = r\"C:\\Users\\henaj\\Downloads\\278.국내 여행로그 데이터(동부권)\\01-1.정식개방데이터\\Training\\01.원천데이터\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "excel_path = os.path.join(save_dir, \"lightGBM-modelling2.xlsx\")\n",
    "\n",
    "# ====== 1. 평가 함수 (요청하신 get_clf_eval) ======\n",
    "def get_clf_eval(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    print(f\"정확도(accuracy): {acc:.4f}\")\n",
    "    print(f\"정밀도(precision, weighted): {precision:.4f}\")\n",
    "    print(f\"재현율(recall, weighted): {recall:.4f}\")\n",
    "    print(f\"F1 score (weighted): {f1:.4f}\")\n",
    "    # return a dict for saving to excel if needed\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# ====== 2. 사용할 컬럼 정의 ======\n",
    "features = ['TRAVEL_COMPANIONS_NUM', 'TRAVEL_STATUS_ACCOMPANY', 'INCOME', 'TRAVEL_STYL_1', 'TRAVEL_MOTIVE_1']\n",
    "target_col = 'TRAVEL_STATUS_DESTINATION'\n",
    "\n",
    "# ====== 3. 데이터 (사용자 환경에 이미 로드된 데이터프레임 사용) ======\n",
    "# 사용자 말에 따르면 df_traveller_ts (12800) 과 df_traveller_vs (1600)가 이미 존재\n",
    "# df_traveller_ts, df_traveller_vs 가 현재 네임스페이스에 있다고 가정\n",
    "# (만약 파일에서 불러와야 하면 pd.read_csv 등으로 불러오세요)\n",
    "\n",
    "# 간단한 확인 (필요시 주석처리)\n",
    "print(\"df_traveller_ts shape:\", df_traveller_ts.shape)\n",
    "print(\"df_traveller_vs shape:\", df_traveller_vs.shape)\n",
    "\n",
    "# ====== 4. 라벨 인코더 준비 (범주형 feature + target) ======\n",
    "# 범주형 features 목록 (문자열 카테고리인 것들)\n",
    "categorical_features = ['TRAVEL_STATUS_ACCOMPANY', 'INCOME', 'TRAVEL_STYL_1', 'TRAVEL_MOTIVE_1']\n",
    "\n",
    "# 복사본 사용 (원본 변경을 피하기 위해)\n",
    "train_df = df_traveller_ts.copy()\n",
    "test_df  = df_traveller_vs.copy()\n",
    "\n",
    "# Instantiate LabelEncoders 저장용 딕셔너리 (나중에 inverse_transform 가능)\n",
    "encoders = {}\n",
    "\n",
    "# 4-1. feature들의 인코딩 (문자열 카테고리 -> 정수)\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    # fit on combined values (train + test) to ensure same encoding across sets (안정성)\n",
    "    combined_vals = pd.concat([train_df[col].astype(str), test_df[col].astype(str)], axis=0)\n",
    "    le.fit(combined_vals)\n",
    "    train_df[col] = le.transform(train_df[col].astype(str))\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "# 4-2. target (여행목적지) 라벨 인코딩\n",
    "le_target = LabelEncoder()\n",
    "combined_target = pd.concat([train_df[target_col].astype(str), test_df[target_col].astype(str)], axis=0)\n",
    "le_target.fit(combined_target)\n",
    "train_df[target_col] = le_target.transform(train_df[target_col].astype(str))\n",
    "test_df[target_col] = le_target.transform(test_df[target_col].astype(str))\n",
    "encoders['target'] = le_target\n",
    "\n",
    "# ====== 5. X / y 분리 및 내부 90:10 split (train-> X_tr/X_val, y_tr/y_val) ======\n",
    "X_full = train_df[features]\n",
    "y_full = train_df[target_col]\n",
    "\n",
    "# 내부 validation (train 데이터에서 10% 떼기), stratify 사용 권장 (클래스 비율 유지)\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_full, y_full, test_size=0.10, random_state=156, stratify=y_full\n",
    ")\n",
    "\n",
    "# 외부 최종 테스트 (제공된 validation 데이터)\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "print(\"X_tr.shape, X_val.shape, X_test.shape:\", X_tr.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "# ====== 6. LightGBM 기본 설정 및 early stopping 으로 학습 (초기 실행) ======\n",
    "num_classes = len(le_target.classes_)\n",
    "base_lgbm = LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=num_classes,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=2000,  # 크게 잡아두고 early stopping으로 멈출 것\n",
    "    random_state=156,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# fit 기본 한 번 실행 (early stopping 확인용) - verbose는 50 간격으로 로그 출력\n",
    "base_lgbm.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_tr, y_tr), (X_val, y_val)],\n",
    "    eval_metric='multi_logloss',   # 다중 클래스의 경우 multi_logloss 권장\n",
    "    early_stopping_rounds=50,\n",
    "    categorical_feature=categorical_features,\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "print(\"base model best_iteration_: \", getattr(base_lgbm, \"best_iteration_\", None))\n",
    "\n",
    "# ====== 7. 하이퍼파라미터 튜닝 (GridSearchCV) ======\n",
    "# 그리드에 넣을 파라미터 범위(예시는 합리적 범위; 필요하면 조정하세요)\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50],\n",
    "    'max_depth': [-1, 10, 20],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# GridSearchCV 객체 (교차검증 10-fold 사용)\n",
    "grid = GridSearchCV(\n",
    "    estimator=base_lgbm,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    cv=10,\n",
    "    verbose=2,\n",
    "    refit=True  # best_estimator_로 재학습(이 옵션 True면 fit 끝난 뒤 best_estimator_가 준비됨)\n",
    ")\n",
    "\n",
    "# GridSearchCV.fit에 추가로 전달할 fit params (LightGBM의 early stopping 등)\n",
    "fit_params = {\n",
    "    \"eval_set\": [(X_tr, y_tr), (X_val, y_val)],\n",
    "    \"eval_metric\": \"multi_logloss\",\n",
    "    \"early_stopping_rounds\": 50,\n",
    "    \"categorical_feature\": categorical_features,\n",
    "    \"verbose\": False\n",
    "}\n",
    "\n",
    "# 주의: sklearn 버전에 따라 GridSearchCV.fit(...)가 추가 fit params를 허용하지 않을 수 있음.\n",
    "# 허용하지 않는 환경이라면, GridSearchCV 없이 수동으로 하이퍼파라미터 루프를 돌려야 합니다.\n",
    "grid.fit(X_tr, y_tr, **fit_params)\n",
    "\n",
    "print(\"GridSearch best params:\", grid.best_params_)\n",
    "print(\"GridSearch best score (CV):\", grid.best_score_)\n",
    "\n",
    "# 최적 모델\n",
    "best_model = grid.best_estimator_\n",
    "print(\"best_model n_estimators (best_iteration_):\", getattr(best_model, \"best_iteration_\", None))\n",
    "\n",
    "# ====== 8. 최종 모델 준비 (전체 training 데이터로 재학습) ======\n",
    "# 보통 내부 validation에서 찾은 best_iteration_를 이용해 전체 train 데이터로 재학습하면 좋음\n",
    "best_iter = getattr(best_model, \"best_iteration_\", None)\n",
    "if best_iter is None:\n",
    "    # best_iteration_가 없다면 best_model의 n_estimators 사용\n",
    "    best_iter = best_model.get_params().get(\"n_estimators\", 200)\n",
    "\n",
    "# 새 모델을 best params로 재생성하여 전체 training(12800)으로 학습\n",
    "final_params = best_model.get_params().copy()\n",
    "# set n_estimators = best_iter for final training\n",
    "final_params['n_estimators'] = best_iter\n",
    "\n",
    "final_model = LGBMClassifier(**final_params)\n",
    "final_model.fit(\n",
    "    X_full, y_full,\n",
    "    categorical_feature=categorical_features,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# ====== 9. 최종 평가 (제공된 validation 데이터에 대해) ======\n",
    "y_pred_test = final_model.predict(X_test)\n",
    "# y_proba_test = final_model.predict_proba(X_test)  # 필요하면 사용\n",
    "\n",
    "print(\"\\n=== 최종 Test 평가 (get_clf_eval) ===\")\n",
    "metrics_dict = get_clf_eval(y_test, y_pred_test)\n",
    "\n",
    "# ====== 10. Feature importance 시각화 및 DataFrame 생성 ======\n",
    "# 중요도 추출 (gain 기준)\n",
    "try:\n",
    "    importances = final_model.booster_.feature_importance(importance_type='gain')\n",
    "    feature_names = final_model.booster_.feature_name()\n",
    "    fi_df = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance_gain\": importances\n",
    "    }).sort_values(by='importance_gain', ascending=False).reset_index(drop=True)\n",
    "except Exception as e:\n",
    "    # fallback: sklearn API로\n",
    "    fi_df = pd.DataFrame({\n",
    "        \"feature\": final_model.feature_name_,\n",
    "        \"importance\": final_model.feature_importances_\n",
    "    }).sort_values(by='importance', ascending=False).reset_index(drop=True)\n",
    "    fi_df.rename(columns={\"importance\": \"importance_gain\"}, inplace=True)\n",
    "\n",
    "print(\"\\nTop feature importances:\\n\", fi_df.head(10))\n",
    "\n",
    "# Plot importance (그래프 저장)\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = plot_importance(final_model, max_num_features=10, importance_type='gain')\n",
    "plt.title(\"LightGBM Feature Importance (gain)\")\n",
    "plt.tight_layout()\n",
    "img_path = os.path.join(save_dir, \"feature_importance.png\")\n",
    "plt.savefig(img_path, dpi=150)\n",
    "plt.close()\n",
    "print(\"Feature importance figure saved to:\", img_path)\n",
    "\n",
    "# ====== 11. 평가 결과 및 중요도 엑셀로 저장 ======\n",
    "# metrics -> DataFrame\n",
    "metrics_df = pd.DataFrame([metrics_dict], index=[\"final_test_eval\"])\n",
    "\n",
    "# confusion matrix (저장용)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index=[f\"true_{c}\" for c in le_target.classes_],\n",
    "                     columns=[f\"pred_{c}\" for c in le_target.classes_])\n",
    "\n",
    "# Save to excel with multiple sheets\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl', mode='w') as writer:\n",
    "    metrics_df.to_excel(writer, sheet_name='metrics', index=True)\n",
    "    fi_df.to_excel(writer, sheet_name='feature_importance', index=False)\n",
    "    cm_df.to_excel(writer, sheet_name='confusion_matrix')\n",
    "    # also save encoding maps (optional) - save target mapping\n",
    "    target_map = pd.DataFrame({\n",
    "        \"label\": list(le_target.classes_),\n",
    "        \"encoded\": list(range(len(le_target.classes_)))\n",
    "    })\n",
    "    target_map.to_excel(writer, sheet_name='target_label_map', index=False)\n",
    "\n",
    "print(\"Results exported to Excel:\", excel_path)\n",
    "\n",
    "# ====== 12. (선택) 인코더 저장 – 나중에 예측 결과를 원래 라벨로 복원하고 싶다면 사용 ======\n",
    "import pickle\n",
    "enc_path = os.path.join(save_dir, \"label_encoders.pkl2\")\n",
    "with open(enc_path, \"wb\") as f:\n",
    "    pickle.dump(encoders, f)\n",
    "print(\"Label encoders saved to:\", enc_path)\n",
    "\n",
    "# 끝\n",
    "print(\"모델링 파이프라인 완료.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
